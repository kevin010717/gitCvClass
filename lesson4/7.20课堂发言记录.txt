00:15:48	唐陈:	1
00:15:49	匡鸿深:	1
00:15:51	深瞳:	1
00:15:51	刘权:	听得到
00:15:53	mpp:	1
00:15:53	陈悦:	o
00:15:58	晔 宣:	1
00:16:01	侠航 陈:	1
00:16:27	Tang JM:	1
00:16:37	wuxi:	1
00:16:52	houzhaoding:	1
00:16:56	肖瑶萍:	1
00:17:02	Brian:	坚持到底
00:17:32	Youki:	赵老师，晚上好！
00:18:23	唐陈:	从入门到放弃
00:18:23	匡鸿深:	逃兵
00:18:24	Youki:	几周后是下一个话题吗？
00:19:01	陈靖韦:	机械学习的各种方法需要掌握吗
00:19:06	Youki:	那现在跟下下下周之间是什么关系？
00:19:14	刘权:	反正根本不指望一次完全听懂。。每次要靠重播1-2次整理框架才能勉强理解。。
00:19:36	Tang JM:	无为而治
00:19:38	Brian:	人生无易事
00:19:45	周富明:	次次上课坐飞机啊
00:20:09	Brian:	。。。
00:20:26	Youki:	你这个同学像尼采
00:20:38	樊建昌:	看不到课件啊
00:20:47	唐陈:	1
00:20:47	陈靖韦:	1
00:20:47	李林洲:	1
00:20:48	houzhaoding:	1
00:20:48	严广宇:	1
00:20:48	Brian:	11
00:20:49	luyz:	可以
00:20:49	夏敏:	1
00:20:49	邓 奇:	1
00:20:50	肖瑶萍:	1
00:20:51	laichengxin:	1
00:20:53	鄢新义:	1
00:20:55	李涛:	老师感觉声音有点小
00:20:57	马玄:	1
00:21:01	李涛:	我电脑声音最大了
00:21:11	强赞霞:	还没开始吗
00:21:15	WW BB:	正在说
00:21:37	樊建昌:	我看到就是个课程介绍
00:21:45	樊建昌:	是这个吗
00:21:51	luyz:	就是一个封面
00:21:52	Youki:	那这周没有要提交的东西吗？
00:21:59	樊建昌:	嗯嗯
00:22:54	Administrator:	没关系，多讲些好
00:22:58	Brian:	多讲面试题呗
00:23:17	陈纬铨:	能给之前作业的答案吗
00:23:41	luyz:	666
00:24:20	luyz:	1
00:24:20	唐陈:	1
00:24:22	邓 奇:	1
00:24:22	houzhaoding:	 1
00:24:22	Tang JM:	no
00:24:38	Youki:	每个回归根本性的不同是h函数的形式吗？
00:24:57	刘宇晴:	我也想要之前作业的答案……
00:25:01	Youki:	每个回归根本性的不同是hypothesis函数的形式吗？
00:27:00	Youki:	谢谢老师。
00:30:02	Administrator:	这里怎么有两个theta1
00:30:48	邓 奇:	.
00:31:56	严广宇:	树突、轴突、细胞体
00:34:42	陈纬铨:	没有
00:34:42	wuxi:	0
00:34:42	陈悦:	1
00:34:43	严广宇:	么有
00:34:43	何川:	no
00:34:43	龙云淋:	没有
00:34:45	luyz:	没有
00:34:45	Tang JM:	感知机？
00:34:46	旭生 孙:	没有
00:34:46	刘向宏:	0
00:34:46	Lenovo:	MEIYOU
00:35:07	唐陈:	激活
00:35:40	Tang JM:	有个激活阈值
00:35:42	Xihui Ju:	阈值
00:35:44	匡鸿深:	噪声抑制
00:35:44	何川:	有激活就有抑制
00:35:48	高格格:	非线性
00:35:48	唐陈:	判断
00:35:48	伍国林:	非线性
00:35:50	陈亚:	筛选信息
00:35:51	严广宇:	非线性变化
00:35:51	qinzongding:	触发作用
00:35:51	陈纬铨:	赋权值
00:35:51	马玄:	非线性
00:35:52	杜琛萍:	非线性
00:35:55	龙云淋:	fexianx
00:36:27	李林洲:	输入与输出不是线性关系
00:36:28	杜琛萍:	除了线性变换以外的变换？
00:36:39	小天-king-北京-1年:	非直线可导
00:36:45	陈亚:	多个输入
00:36:45	Tang JM:	不是一一对应的
00:36:52	龙云淋:	不能通过加权
00:40:50	Youki:	那现在的话，每个 activation  function应该都不一样吧？
00:41:19	Administrator:	是不是每个神经元就有一个激活函数？
00:41:24	严广宇:	是的
00:41:41	Tang JM:	不一定吧
00:42:10	龙云淋:	不一定
00:42:54	严广宇:	1
00:42:54	陈纬铨:	ok
00:42:55	陈悦:	ok
00:42:55	李林洲:	1
00:42:56	wuxi:	1
00:42:56	邓 奇:	1
00:42:57	Tang JM:	·
00:42:57	唐陈:	1
00:42:57	luyz:	ok
00:42:58	肖瑶萍:	1
00:42:59	鄢新义:	 1
00:42:59	龙云淋:	OK
00:43:27	Youki:	那如果每个神经元都有 activation  function，那不是这个网络就没有线性能力了？
00:43:45	huruiqi:	老师 是不是少了一个bias
00:43:51	Youki:	哦哦。
00:44:16	Youki:	就是说， activation  function既可以线性也可以非线性，是一种全能的函数吗？
00:44:46	陈亚:	可以你试验的效果好 爱用啥用啥
00:44:51	Tang JM:	激活函数只是模型中的一部分，模型可以拟合非线性和线性的
00:45:15	Yimin MA:	X0是哪里来的？
00:45:30	陈亚:	一般激活函数 分类和回归是有经验函数的
00:45:38	Tianhu Zhang:	X0 bias
00:45:41	Tang JM:	x0 就是bias
00:45:48	陈亚:	x0是常数
00:46:00	严广宇:	no
00:46:01	马玄:	需要
00:46:03	李林洲:	图上有三个输入,公式里有怎么有四个
00:46:04	刘志强:	X0 bias
00:46:06	Administrator:	要
00:46:19	孟明:	可以认为x0=1
00:46:23	Youki:	这个括号里面的“+”是什么意思呀？
00:46:25	huruiqi:	哦哦 看到了
00:46:30	Youki:	难道是直接相加？
00:46:46	彭冰:	就是 直接 相加
00:46:49	huruiqi:	刚刚没注意公式 只看了图
00:46:51	严广宇:	是的
00:46:53	孟明:	就是加法运算
00:47:02	小天-king-北京-1年:	第一次看这些公式，一点也看不懂
00:47:04	包颖:	X0=1，seta_i0=bias
00:47:11	孙小婷:	为什么需要偏置？
00:47:12	彭冰:	矩阵乘法 懂了吧
00:47:26	Youki:	直接相加为什么不干脆用一个参数？
00:47:39	qinzongding:	看不懂
00:47:43	Youki:	为什么还要分成4个呢？
00:47:43	严广宇:	输入有多个啊，如何用一个参数？
00:47:44	陈亚:	就是简单的权值* 信息值相加 在输入激活函数
00:47:45	马玄:	θ下标是什么意思？
00:47:49	yty:	g时什么
00:47:50	luyz:	这不还是线性的？
00:47:53	龙云淋:	每个特征所占的比重不同啊
00:47:57	Youki:	哦哦，看错了，不好意思。
00:48:03	龙云淋:	g就是激活函数
00:48:04	Yimin MA:	g() 是啥？
00:48:09	严广宇:	g()就是激活函
00:48:10	qinzongding:	x0是什么
00:48:13	孟明:	就是激活函数
00:48:15	Youki:	谢谢同学们。
00:48:22	Administrator:	这个layer3 里面的 a0(2) 是啥
00:48:41	Tang JM:	a02 是第二层的bias
00:49:31	Brian:	sigmoid
00:49:37	唐陈:	relu
00:49:38	龙云淋:	relu
00:49:39	Tang JM:	max()
00:49:39	肖瑶萍:	relu
00:49:39	Lenovo:	tanh
00:49:40	陈亚:	softmax
00:49:43	Brian:	softmax
00:49:43	匡鸿深:	激活函数的选取有什么讲究吗
00:49:48	严广宇:	tanh
00:49:55	陈亚:	hinge
00:50:17	Youki:	现在激活函数的提出还是研究热点吗？
00:50:39	严广宇:	01
00:50:40	Brian:	01
00:50:48	严广宇:	-1~1
00:50:50	唐陈:	-1,1
00:51:25	唐陈:	1
00:51:26	Tang JM:	1
00:51:28	肖瑶萍:	1
00:51:35	houzhaoding:	拼写没看清楚。
00:51:41	韦俊:	tanh
00:52:28	龙云淋:	为什么要bias
00:53:27	唐陈:	16
00:53:27	陈亚:	补齐拟合偏差
00:54:42	唐陈:	1
00:54:57	Eric:	RNN和CNN的区别就是激活函数不同吗？
00:55:08	严广宇:	不是
00:55:17	严广宇:	RNN有循环结构
00:55:27	陈纬铨:	懂了
00:55:29	严广宇:	1
00:55:44	Administrator:	老师，看不到你的笔
00:56:02	孙旭生:	x1 是 一个样本的其中一个特征吗
00:56:24	邓 奇:	X1*theta11  代表什么？每个输入的权重吗？
00:57:23	吴彬:	1
00:57:40	匡鸿深:	是上一层的输入乘以权重的和？
00:58:13	wuxi:	1
00:58:15	李林洲:	1
00:58:16	孟乒乒:	1
00:58:17	唐明利:	1
00:58:17	小天-king-北京-1年:	x1是房子的平米数，x2是房间数，可以这样理解吧？
00:58:42	孙旭生:	x1 x2 x3 都属于一个样本是吧
00:59:16	小天-king-北京-1年:	a1，a2，a3是不同的激活函数吗？
00:59:35	孙旭生:	那样x1就变成了一个100维的向量了吗
01:00:13	Eric:	既然激活函数不一样，为什么都用符号g呢？
01:00:18	李涛:	1
01:00:20	侠航 陈:	1
01:00:34	严广宇:	只是一个字母表示而已……
01:00:46	Eric:	好的，谢谢老师。
01:00:51	邓 奇:	1
01:00:55	龙云淋:	1
01:00:58	唐陈:	1
01:01:00	唐陈:	1
01:01:00	陈悦:	可以
01:01:00	斯太康:	1
01:01:01	唐陈:	1
01:01:01	邓 奇:	可以
01:01:01	孟乒乒:	1
01:01:01	李林洲:	可以
01:01:02	吴彬:	1
01:01:07	孙小婷:	1
01:01:07	严广宇:	这就和未知变量用x,结果用y表示一样
01:01:08	小天-king-北京-1年:	如果a1，a2.。的激活函数都相同layer2的数据是不是相同的啊？
01:01:09	樊建昌:	看不见屏幕啊
01:01:12	Yimin MA:	这里显示的是NN的结构，所以训练就是要找thita的值吧？激活函数是自己选的，我的理解对吗？
01:01:19	樊建昌:	黑屏
01:01:25	tongzongjie:	没声音了？
01:01:31	qinzongding:	卡啦吗
01:01:31	严广宇:	没了
01:01:33	Brian:	？？
01:01:34	qinzongding:	看不到了
01:01:35	陈纬铨:	真的掉了
01:01:35	唐陈:	dianlo 
01:01:35	孙旭生:	真的掉了
01:01:35	周富明:	这就掉下去了？
01:01:35	高格格:	掉了
01:01:36	薛毓铨:	掉了真的
01:01:37	陈悦:	老师预感到自己卡了
01:01:37	luyz:	掉了？
01:01:37	李涛:	老师真掉下去了
01:01:37	韦俊:	流了
01:01:37	Eric:	黑屏了。
01:01:38	罗家伟:	刚说完就掉了
01:01:38	yangfanqihang:	?
01:01:38	孙小婷:	。。
01:01:39	薛毓铨:	哈哈哈哈
01:01:39	陈靖韦:	说完就没了
01:01:41	李涛:	哈哈
01:01:42	周富明:	这。。
01:01:43	WW BB:	稍等囧
01:01:43	薛毓铨:	这、
01:01:44	luyz:	说掉就掉
01:01:44	龙云淋:	话音刚落
01:01:45	Brian:	预言家
01:01:45	李涛:	牛逼
01:01:45	houzhaoding:	准时
01:01:46	luyz:	666
01:01:47	陈纬铨:	神预言
01:01:47	韦俊:	遛了溜了
01:01:48	龙云淋:	预言家
01:01:49	薛毓铨:	太真实了
01:01:49	邓 奇:	掉了
01:01:49	孟乒乒:	刚说完就掉了
01:01:50	Brian:	牛叉
01:01:52	luyz:	有牌面
01:01:59	Lenovo:	....
01:02:01	houzhaoding:	天黑请闭眼
01:02:01	claireliu:	下课了？
01:02:01	Brian:	王牌预言家
01:02:02	孟乒乒:	老师真准
01:02:03	严广宇:	怕是断网了
01:02:04	龙云淋:	说断就断
01:02:04	WW BB:	稍等
01:02:04	Yimin MA:	终于看到老师传说中的“掉了”
01:02:06	laichengxin:	掉线了嘛
01:02:19	斯太康:	神预测
01:02:23	Brian:	come back
01:02:29	huruiqi:	掉的太快就像龙卷风
01:02:35	樊建昌:	妥了
01:02:37	陈纬铨:	来了
01:02:37	薛毓铨:	说掉就掉
01:02:38	高格格:	来了
01:02:41	唐陈:	1
01:02:41	严广宇:	1
01:02:42	邓 奇:	1
01:02:43	陈纬铨:	可以
01:02:43	樊建昌:	可以了
01:02:43	Administrator:	1
01:02:43	斯太康:	1
01:02:43	孟乒乒:	1
01:02:44	laichengxin:	1
01:02:44	luyz:	可以了
01:02:47	深瞳:	1
01:02:47	Brian:	突然暴风雨
01:02:48	Yimin MA:	1
01:02:58	Tang JM:	老师，如果用矩阵计算，是不是a1,a2,a3应该用相同的激活函数。
01:03:16	小天-king-北京-1年:	如果a1，a2.。的激活函数都相同layer2的数据是不是相同的啊？
01:03:28	Eric:	好的，谢谢老师。
01:03:40	陈悦:	没了
01:03:43	孙旭生:	没有
01:03:45	陈纬铨:	没了
01:03:47	马玄:	1
01:03:51	houzhaoding:	1
01:03:53	Tang JM:	laye2的激活函数 一般是sigmoid或者softmax
01:04:52	樊建昌:	继续
01:04:53	严广宇:	继续
01:04:55	Tang JM:	1
01:04:56	Yimin MA:	这里显示的是NN的结构，激活函数是确定的，所以训练就是要找thita的值吧？我的理解对吗？
01:04:56	龙云淋:	继续
01:04:56	邓 奇:	1
01:04:57	孟乒乒:	1
01:05:03	陈华涓:	老师，神经元的个数一般是多少呢
01:05:37	罗家伟:	跟特征向量的个数有关吧
01:06:12	Yimin MA:	所以不是越多越好？
01:07:00	龙云淋:	越多越好的话，极限就是穷举了，没有意义
01:07:05	Tang JM:	与
01:07:14	Tang JM:	逻辑与
01:07:17	严广宇:	按位与
01:08:29	严广宇:	0
01:08:50	wuxi:	g(-30+20*x1+20*x2)
01:08:52	樊建昌:	加权和
01:08:56	杜琛萍:	g(-30+20x1+20x2)
01:08:56	严广宇:	sigmoid(-30+0+20)=sigmoid(10)
01:09:26	严广宇:	x1不是0吗
01:10:57	Brian:	0
01:10:58	肖瑶萍:	1
01:10:58	wuxi:	1
01:10:59	郑兴纯:	0
01:11:00	邓 奇:	1
01:11:01	陈纬铨:	没有
01:11:01	houzhaoding:	1
01:11:01	Youki:	神奇！
01:11:06	Youki:	叹为观止！
01:11:32	孙小婷:	这个是不是就是逻辑回归？
01:11:52	陈华涓:	一层的是
01:12:22	Administrator:	有
01:12:44	李涛:	老师已经写了 x1 = -10
01:13:20	孟明:	1 1 1就可以
01:13:20	强赞霞:	g(-10),g(10),g(10),g(30)
01:14:11	Brian:	1
01:14:53	Tang JM:	1
01:14:54	严广宇:	1
01:14:56	肖瑶萍:	1
01:14:57	邓 奇:	1
01:14:57	张帅:	1
01:14:58	侠航 陈:	1
01:14:58	唐陈:	1
01:14:59	陈纬铨:	懂了
01:14:59	Tang JM:	再来个非
01:15:01	luyz:	1
01:15:17	laichengxin:	老师，是通过西塔的值来控制与或非吗
01:15:22	houzhaoding:	这个与θ的大小没关系吗
01:15:27	马玄:	theta参数随便给的吗？
01:15:29	严广宇:	有关系、
01:15:40	旭生 孙:	这就是权重来决定结果啊
01:15:57	luyz:	10
01:16:01	邓 奇:	Theta 参数决定结果
01:16:02	严广宇:	为了拟合出这样的结果，需要进行多次梯度更新
01:16:13	Tang JM:	theta 只要满足要求可以随便取，老师举个栗子
01:16:13	龙云淋:	theta就是实际中要去训练得到的
01:16:18	wuxi:	是10，-20
01:16:22	陈纬铨:	10  -20
01:16:24	罗家伟:	没搞错
01:16:29	罗家伟:	是非x1
01:16:38	罗家伟:	非x1 = 1
01:16:55	Yimin MA:	theta是应该根据样板训练出来的吧？这三个例子是不是想说明同一个模型用不同theta就能完美拟合现实中的三种情况。
01:17:28	陈靖韦:	训练
01:17:28	唐陈:	不是训练的吗
01:17:32	luyz:	训练出来的
01:17:59	严广宇:	只要网络足够复杂，神经网络理论上可以拟合任何函数
01:18:12	孙小婷:	theta是不是可能有很多组取值？
01:18:52	孙小婷:	ok
01:19:25	wuxi:	复杂怎么体现？
01:19:31	Yimin MA:	复杂=theta越多？
01:19:35	唐陈:	层数
01:20:00	匡鸿深:	不是说不是神经元越多越好吗
01:20:17	luyz:	够用就行
01:20:29	陈纬铨:	过多会过拟合
01:21:00	huruiqi:	那神经元个数怎么确定呢
01:21:23	刘向宏:	是不是类似泰勒展开式如果取越多项，就越贴近准确函数
01:21:26	樊建昌:	经验吧
01:21:31	Jone Liu:	主要靠策略
01:21:55	Yimin MA:	神经元的单元数量是个还是层？
01:21:55	Jone Liu:	炼丹师
01:22:02	孟明:	深度指的是层数多吗
01:22:16	严广宇:	是的
01:22:17	樊建昌:	两个因素 
01:22:20	龙云淋:	深度和宽度
01:22:35	樊建昌:	1
01:22:36	Yimin MA:	1
01:23:04	luyz:	线性
01:23:24	houzhaoding:	只有线性。
01:23:29	樊建昌:	线性
01:23:34	Jone Liu:	线性
01:23:44	樊建昌:	结果范围会不确定
01:24:39	杜琛萍:	层数就没有意义了
01:24:50	杜琛萍:	最终结果和一层的是一样的
01:25:11	樊建昌:	同意楼上。
01:25:20	严广宇:	是的
01:25:51	luyz:	1
01:25:51	邓 奇:	1
01:25:51	wuxi:	非线性产生的“生命”
01:25:53	龙云淋:	1
01:25:54	匡鸿深:	1
01:25:55	鄢新义:	1
01:26:13	唐陈:	1
01:26:14	wuxi:	1
01:26:14	Jone Liu:	1
01:26:15	陈悦:	1
01:26:16	Tianhu Zhang:	1
01:26:17	claireliu:	1
01:26:17	Xihui Ju:	1
01:26:18	陈纬铨:	过
01:26:18	张帅:	1
01:26:18	Yimin MA:	1
01:26:18	侠航 陈:	1
01:32:10	樊建昌:	1
01:32:11	郑兴纯:	1
01:32:11	严广宇:	1
01:32:12	邓 奇:	在
01:33:37	韦俊:	这么看bias还是有用？
01:33:48	旭生 孙:	看是能看懂 自己想出来就难了
01:34:14	彭冰:	因为输入少且值比较少 +某一个值就显得很重要
01:34:21	Jone Liu:	反过来很难推
01:34:36	小天-king-北京-1年:	x1&x2和！x1&！x2是什么区别啊？
01:34:54	邓 奇:	这样a1 a2激活函数就不一样了是吗？
01:35:14	匡鸿深:	改变这几个合并过程会对权重造成影响吗
01:35:25	houzhaoding:	一样
01:35:26	陈靖韦:	补集
01:35:42	wuxi:	1
01:35:43	李林洲:	1
01:35:44	陈悦:	0
01:35:45	邓 奇:	1
01:35:45	陈靖韦:	1
01:35:46	陈纬铨:	没有
01:35:48	龙云淋:	没有
01:35:48	韦俊:	1
01:35:48	laichengxin:	过
01:35:49	claireliu:	1
01:35:50	Jone Liu:	1
01:35:51	houzhaoding:	老师怎么想出来的
01:35:52	侠航 陈:	1
01:36:02	罗家伟:	给定输入值的不同权值经过sigmoid可以实现各种逻辑操作
01:36:13	huruiqi:	我也想问刚刚是怎么想出来的
01:36:44	张帅:	刚刚是那个异或怎么想出来的
01:36:57	唐陈:	概率
01:37:03	龙云淋:	类似于逻辑电路设计。。
01:37:22	Jone Liu:	这个比喻很形象
01:38:25	鄢新义:	0，0.6，0.4，0
01:39:01	houzhaoding:	为啥加起来不是1
01:39:07	鄢新义:	是1
01:39:10	陈纬铨:	是1
01:39:11	鄢新义:	没错
01:39:28	鄢新义:	是0.33 有点像0.35
01:39:29	李林洲:	softmax
01:39:29	唐陈:	softmax
01:39:30	Tang JM:	softmax
01:39:30	夏敏:	softmax
01:39:37	龙云淋:	softmax
01:39:49	陈纬铨:	多分类用softmax
01:39:55	Youki:	好的，谢谢老师。
01:39:58	Tang JM:	exp/sum(exp)
01:41:19	wuxi:	1
01:41:19	陈靖韦:	1
01:41:20	唐陈:	1
01:41:20	houzhaoding:	1
01:41:22	luyz:	1
01:41:22	Jone Liu:	1
01:41:22	马玄:	0
01:41:23	李林洲:	1
01:41:36	luyz:	1
01:41:37	严广宇:	1
01:41:41	laichengxin:	好的
01:41:43	邓 奇:	好
01:41:45	李林洲:	hao
01:41:46	龙云淋:	hao
01:41:47	WW BB:	9:35回来
01:41:49	刘权:	oh no..
01:42:13	刘权:	没有。。就是觉得数学很难。。
01:42:27	Jone Liu:	这个公式看不懂
01:42:37	Tang JM:	好像在哪见过
01:42:44	严广宇:	这个就是上节课的公式啊
01:42:47	wuxi:	逻辑回归啊
01:42:50	刘权:	逻辑回归有点像
01:42:53	wuxi:	sigmoid
01:43:01	匡鸿深:	老师，抛开面试，实际训练中用到的数学推理多吗
01:43:31	Jone Liu:	面试造火箭，工作拧螺丝
01:43:38	luyz:	666
01:43:38	樊建昌:	老师刚刚讲的那个与啥意思啊
01:43:50	樊建昌:	x1,与x2
01:44:00	刘权:	一般应该都是调参侠
01:44:11	孟明:	梯度下降应该是最重要的数学推导了
01:44:22	严广宇:	现在公司不要调包侠了……
01:44:49	Jone Liu:	好的调包侠也是很少的
01:44:54	houzhaoding:	是不是解决线性回归有好多种方法？例如最小二乘法，梯度下降法，
01:45:04	匡鸿深:	这么说有点方啊
01:45:05	houzhaoding:	还是说本质都一样
01:45:24	孟明:	公司里就是选模型调参数？
01:45:43	Tang JM:	老师有时间可以介绍下老师每天的工作内容是啥。
01:45:45	严广宇:	你把公司想得的也太简单了
01:46:11	严广宇:	如果只是调包，门槛太低了
01:46:16	刘权:	哪有这么多功夫让你造轮子。。拿别人的轮子不是正常操作么。。最多改改轮子
01:46:19	匡鸿深:	选模型是说现在图像分类、目标检测啥的都有现成的了，平时就是调调参数完事？
01:46:30	Jone Liu:	干不过业务......
01:47:11	Jone Liu:	不落地，没有实际价值，基本没有认可度
01:47:32	严广宇:	是啊，公司要的是实际解决问题的人……
01:47:41	樊建昌:	想要问下大家
01:47:53	樊建昌:	老师刚刚讲得那个x1,x2的与或啥意思
01:47:55	严广宇:	现有的模型，直接应用到实际问题很多。
01:48:01	旭生 孙:	那到底是研发新东西还是用已有的技术做想要的啊
01:48:10	严广宇:	公司要的，是如何解决这些问题的人
01:48:18	Jone Liu:	可以建议老师以后有时间，专门解答一下
01:48:38	陈华涓:	老师，神经网络调的时候是不是变动牵连大，比较难调
01:48:42	高格格:	老师，课间问一下，我们之后会说到C++的opencv么？用的时候碰到了一些上网搜也没找答案的莫名其妙的error...
01:49:09	匡鸿深:	听大家这么一说感觉所有公司用的都一样，只是训练数据不同而已
01:49:20	樊建昌:	不一样的
01:49:26	匡鸿深:	怎么感觉门槛这么低的
01:49:31	樊建昌:	同样一个网络衣蛾模型能用好多场景
01:49:39	严广宇:	如果你这么想，一面你就直接死了
01:49:41	樊建昌:	但是调参方式方法不一样的
01:49:48	樊建昌:	有区别的
01:49:50	邓 奇:	不需要自己建模吗？如果遇到新的问题怎么办，现有模型解决不了的
01:49:53	樊建昌:	等干起项目就知道啦
01:50:23	樊建昌:	得把数据集先搞好。再说模型的事情
01:50:30	严广宇:	比如，数据就那么多，模型效果一直很差一直过拟合，怎么办？
01:50:31	樊建昌:	没有那么简单的
01:50:36	樊建昌:	1
01:50:38	旭生 孙:	1
01:50:39	樊建昌:	在
01:50:39	严广宇:	1
01:50:40	邓 奇:	1
01:50:41	陈悦:	ok
01:50:41	luyz:	1
01:50:42	刘权:	在呀
01:50:42	龙云淋:	1
01:50:43	houzhaoding:	1
01:50:44	唐陈:	1
01:51:32	唐陈:	达摩院
01:52:14	樊建昌:	大佬负责涉及。我们负责用。
01:52:30	樊建昌:	架构师和码农的区别？
01:52:35	唐陈:	包肯定是要调的
01:52:43	小天-king-北京-1年:	大人物忙不过来，小人模仿，大人物造工具，小人物使用
01:52:48	Tang JM:	老师有时间可以介绍下，老师每天的工作内容是啥。
01:52:50	Jone Liu:	我感觉有点像
01:52:58	Tang JM:	。。。。。。。
01:53:14	陈靖韦:	老师哪个公司的
01:53:22	Tang JM:	保密
01:53:23	houzhaoding:	专人负责吗
01:53:55	郑兴纯:	感觉要在嵌入式设备端实现 更多考虑的是量化 减小模型
01:54:29	严广宇:	是的，模型压缩是必然要做的
01:54:47	吴彬:	这个同学有推荐么
01:54:50	Youki:	Cool!
01:54:51	唐陈:	王者
01:54:56	Youki:	CNN!
01:55:22	Youki:	场景，Cool！
01:55:55	孟乒乒:	最近过了一遍常用的数据结构和算法，准备刷题了，但是感觉过的数据结构和算法就忘完了，不知道是再看一遍，还是直接刷题，边刷边学。每天都在纠结，求指导呀老师
01:56:00	陈华涓:	调参有什么建议么，调到有点累😯
01:56:00	Youki:	我相信你，老师声音好听！
01:56:11	Jone Liu:	1
01:56:11	luyz:	 1
01:56:12	樊建昌:	1
01:56:15	侠航 陈:	1
01:56:15	邓 奇:	1
01:56:43	陈华涓:	感觉神经网络挺难调
01:57:41	Youki:	因为我以后用CNN。
01:57:46	Youki:	这个古老了。
01:57:46	樊建昌:	哈哈
01:57:52	李林洲:	这个loss用的不多
01:57:53	Tang JM:	历史原因
01:58:04	李涛:	没办法手推
01:58:47	Tang JM:	Hinton
02:00:09	Tang JM:	链式求导
02:00:11	houzhaoding:	0
02:00:12	tongzongjie:	0
02:00:12	唐陈:	链式
02:00:12	邓 奇:	不知道
02:00:14	Lenovo:	链式法则
02:00:14	Youki:	我不知道。
02:00:15	龙云淋:	链式法则
02:00:15	严广宇:	链式求导法则
02:00:19	小天-king-北京-1年:	不知道
02:00:20	ray 王睿:	不知道
02:00:25	Brian:	高数里面的
02:02:06	樊建昌:	1
02:02:08	刘权:	这个。。用都会用呀
02:02:09	Jone Liu:	1
02:02:11	肖瑶萍:	1
02:02:12	严广宇:	1
02:02:12	Administrator:	1
02:02:13	夏敏:	1
02:02:13	tongzongjie:	1
02:02:16	马玄:	1
02:02:42	樊建昌:	1
02:02:46	夏楠:	1
02:04:09	樊建昌:	1
02:04:19	陈靖韦:	0
02:04:20	马玄:	0
02:04:25	赵玉刚:	1
02:04:25	Youki:	多个NN相加吗？
02:04:36	马玄:	不懂
02:04:38	李涛:	不懂
02:04:41	Youki:	F和Q之间什么关系？
02:04:42	严广宇:	多个不同梯度相加？
02:04:44	刘权:	没明白，qmn是啥
02:04:50	樊建昌:	M,Q是各个神经元的抽象
02:04:52	樊建昌:	？
02:04:56	Youki:	懂了。
02:04:59	孟明:	懂
02:05:03	李林洲:	懂
02:05:03	luyz:	1
02:05:04	houzhaoding:	1
02:05:04	刘权:	懂
02:05:06	邓 奇:	可以
02:05:09	樊建昌:	1
02:05:42	匡鸿深:	体现在网络上是怎样的
02:05:55	樊建昌:	同层的神经元的输入
02:05:57	Youki:	A和B的激活函数不同，对吧？
02:06:03	樊建昌:	对
02:06:06	wuxi:	L = L1+L2+L3，L1 = Fn(Fn-1(Fn-1…
02:06:20	严广宇:	1
02:06:31	严广宇:	明白
02:06:57	李林洲:	懂
02:06:58	邓 奇:	1
02:06:59	刘权:	懂了。。
02:06:59	龙云淋:	1
02:07:00	马玄:	懂了
02:07:02	Tang JM:	GoogleNet 里面多个loss 这种形式么
02:07:43	Tang JM:	1
02:07:44	匡鸿深:	1
02:07:45	樊建昌:	1
02:07:47	龙云淋:	1
02:07:47	Jone Liu:	1
02:07:49	刘向宏:	1
02:08:56	唐陈:	代进去再求
02:10:33	龙云淋:	1
02:10:33	陈悦:	1
02:10:34	韦俊:	1
02:10:34	李林洲:	是x
02:10:34	刘权:	可以
02:10:35	严广宇:	可以
02:10:35	唐陈:	1
02:10:36	Jone Liu:	1
02:10:36	luyz:	1
02:10:36	樊建昌:	1
02:10:37	孔维昌-cv:	1
02:10:38	马玄:	1
02:10:40	郑兴纯:	x
02:10:43	严广宇:	这就是计算图
02:10:56	李林洲:	公式里是x
02:11:44	qinzongding:	q对z 的导？
02:11:57	唐陈:	1
02:11:58	包颖:	1
02:12:09	樊建昌:	老师写错了
02:12:14	樊建昌:	是z不是x
02:12:36	包颖:	af/fz
02:12:38	李林洲:	右边求导公式
02:12:38	qinzongding:	链式求导
02:13:52	Tang JM:	1
02:13:55	包颖:	链式，记得乘上上一个结果
02:13:56	严广宇:	1
02:13:59	龙云淋:	1
02:14:08	严广宇:	乘1
02:14:10	李林洲:	根据输入权重
02:14:10	houzhaoding:	同时共享
02:15:30	龙云淋:	1
02:15:33	刘向宏:	1
02:15:41	包颖:	这是权值共享么？
02:15:42	严广宇:	1
02:15:49	Youki:	交换具体怎么体现的？
02:16:04	陈靖韦:	分发就是一样的吗
02:16:20	包颖:	为什么说分发，不说是一样的loss
02:16:26	包颖:	哦哦
02:16:35	houzhaoding:	x,y系数不同应该分发不一样吧
02:16:40	旭生 孙:	为什么说是 交换loss呢 哪来的loss啊
02:16:58	Tang JM:	这个结论是有什么指导性作用吗？
02:16:59	陈靖韦:	这整体是在计算loss
02:17:22	Administrator:	loss是损失函数吗
02:17:40	luyz:	cost function
02:17:53	包颖:	参数没有系数么？就是theta
02:17:55	李林洲:	是交换和分发的是loss还是导数
02:18:15	陈靖韦:	1
02:18:44	严广宇:	1
02:18:44	唐陈:	就反向传递的这个loss?
02:19:04	匡鸿深:	加法的分发是均分吗
02:19:22	严广宇:	不是
02:19:29	唐陈:	1
02:19:33	刘权:	加法的共享体现在哪里。。好像只看到相等。。但没看到共享
02:19:44	陈靖韦:	就是假设f是在计算loss，然后把loss分别传导到xyz里面吧
02:19:47	wuxi:	要指导更新theta
02:19:49	旭生 孙:	通过求导的方式来传递吗
02:19:54	包颖:	也就是通过BP看来每个参数x对loss的影响。知道影响后才好调整theta
02:20:22	严广宇:	1
02:20:25	唐陈:	1
02:20:26	邓 奇:	1
02:20:27	龙云淋:	1
02:20:28	陈悦:	明白
02:20:28	luyz:	1
02:20:29	陈靖韦:	1
02:20:29	Jone Liu:	1
02:20:41	Youki:	往后讲吧，用神经网络的例子讲一下。
02:24:06	Youki:	-1/2
02:24:11	罗家伟:	2
02:25:01	luyz:	1
02:25:02	陈靖韦:	哪一块
02:25:17	包颖:	乘法交换
02:25:18	陈悦:	-4
02:25:20	Administrator:	2
02:25:30	tongzongjie:	2
02:25:33	肖瑶萍:	2
02:25:38	罗家伟:	-8，-6
02:25:57	tongzongjie:	-12
02:26:04	李林洲:	2
02:26:45	houzhaoding:	max怎么回传的
02:26:50	Administrator:	吗
02:26:52	Administrator:	没
02:26:59	包颖:	看哪个大吧
02:27:02	qinzongding:	传的是权重还是／
02:27:24	houzhaoding:	1
02:27:29	qinzongding:	这个是怎么加怎么乘的，没看明白
02:28:17	Administrator:	B是哪块
02:28:52	luyz:	1
02:28:53	陈纬铨:	这个懂
02:28:53	邓 奇:	1
02:29:41	qinzongding:	如果是3个呢，怎么换？
02:30:10	qinzongding:	11
02:30:12	tongzongjie:	1
02:30:13	luyz:	1
02:30:15	邓 奇:	1
02:30:18	李林洲:	1
02:30:19	houzhaoding:	1
02:30:27	陈靖韦:	每个函数输入只能有两个是吧
02:30:37	罗家伟:	最小的单元就只是乘法和加法和max?
02:31:10	罗家伟:	1
02:31:29	Tang JM:	sigmoid么
02:32:23	houzhaoding:	1
02:32:23	严广宇:	1
02:32:25	邓 奇:	1
02:32:27	李林洲:	1
02:32:29	侠航 陈:	1
02:32:50	旭生 孙:	计算的时间
02:33:01	龙云淋:	梯度消失？
02:33:07	马玄:	loss太大
02:33:29	Administrator:	学习率太大？？
02:33:29	Brian:	sigmoid
02:33:53	包颖:	因为链式法则累乘了
02:33:54	龙云淋:	层数太深，梯度值太小
02:33:55	夏敏:	网络导数太深
02:34:00	夏敏:	层数
02:34:32	Tang JM:	乘上的激活函数小于1
02:35:07	陈悦:	上一层的加权
02:35:07	马玄:	前一个神经元的输出
02:35:09	李林洲:	每层线性运算结果
02:35:21	Administrator:	x，a1(x),a2(a1(x))
02:37:01	tongzongjie:	1
02:37:02	luyz:	1
02:37:03	wuxi:	1
02:37:03	李林洲:	1
02:37:03	孔维昌-cv:	1
02:37:03	邓 奇:	1
02:37:03	陈悦:	1
02:37:04	夏敏:	1
02:37:04	严广宇:	1
02:38:35	luyz:	1
02:38:41	唐陈:	1
02:38:45	陈华涓:	min loss
02:38:45	李林洲:	1
02:38:47	孙小婷:	1
02:40:16	Administrator:	1
02:40:17	luyz:	1
02:40:21	王睿:	1
02:40:23	李小飞:	1
02:40:24	Tang JM:	1
02:41:29	高格格:	1
02:42:20	Tang JM:	1
02:42:24	龙云淋:	1
02:43:50	luyz:	1
02:43:51	郑兴纯:	1
02:43:52	邓 奇:	1
02:43:52	马玄:	1
02:43:53	包颖:	1
02:43:55	李小飞:	1
02:43:55	孙小婷:	1
02:44:51	luyz:	1
02:44:52	高格格:	1
02:44:56	严广宇:	1
02:45:46	Tang JM:	1
02:45:47	luyz:	1
02:46:09	wuxi:	断了
02:46:11	陈悦:	趋近0
02:46:12	Tianhu Zhang:	不稳定
02:46:15	Tang JM:	激活函数的导数小于1
02:46:17	陈华涓:	不可导情况
02:46:21	严广宇:	梯度消失？
02:46:22	houzhaoding:	放大误差？
02:46:28	Brian:	daoshu0
02:46:31	Brian:	倒数为零
02:46:36	李林洲:	指数关系
02:49:09	严广宇:	0
02:49:11	wuxi:	0
02:49:12	李林洲:	0
02:49:12	tongzongjie:	0
02:49:12	luyz:	0
02:49:13	龙云淋:	0
02:49:18	夏敏:	0
02:51:36	Tang JM:	1
02:51:39	houzhaoding:	1
02:51:39	luyz:	1
02:51:39	旭生 孙:	11
02:51:40	wuxi:	soga，学习了
02:51:40	陈悦:	1
02:51:41	李小飞:	1
02:51:43	侠航 陈:	1
02:51:43	陈华涓:	1
02:51:43	李林洲:	1
02:51:44	龙云淋:	1
02:51:44	王睿:	1
02:51:46	tongzongjie:	1
02:51:55	Jone Liu:	1
02:52:56	houzhaoding:	丧失非线性
02:53:03	陈华涓:	捕捉不到非线性
02:54:12	luyz:	越来越大
02:55:01	wuxi:	这还咋玩
02:55:10	luyz:	1
02:55:12	Youki:	那怎么办？
02:55:12	陈纬铨:	懂了
02:55:21	严广宇:	BN
02:55:28	龙云淋:	换激活函数
02:55:37	Tang JM:	用RELU
02:55:43	houzhaoding:	这个东西有人用吗？
02:56:35	陈华涓:	但是BP不是一直往减少loss方向么，为什么会增呢
02:57:02	邓 奇:	有一个问题，这个例子 F和loss是什么关系？
02:58:02	严广宇:	1
02:58:04	Tianhu Zhang:	1
02:58:07	yty:	a3,a2,a1都是激活函数吗
02:58:08	孔维昌-cv:	1
02:58:09	孙小婷:	1
02:58:24	张帅:	theta2和theta3也是训练出来的么
02:58:56	张帅:	1
02:59:05	严广宇:	1
02:59:07	陈悦:	1
02:59:09	邓 奇:	1
02:59:17	luyz:	继续吧
02:59:17	Brian:	不用了
02:59:18	陈华涓:	继续
02:59:20	杜琛萍:	继续吧
02:59:26	邓 奇:	继续
02:59:28	龙云淋:	继续
02:59:30	严广宇:	继续吧
02:59:32	孙小婷:	继续
03:00:58	Brian:	过拟合？
03:01:00	Tang JM:	过拟合
03:01:10	Tang JM:	噪声铭感
03:01:28	龙云淋:	泛化能力不足
03:01:32	Brian:	学习能力变差
03:02:39	陈悦:	1
03:02:42	陈纬铨:	没有
03:02:42	邓 奇:	1
03:03:15	Tang JM:	验证集loss高
03:04:44	夏敏:	异常点
03:06:24	wuxi:	1
03:06:24	Brian:	1
03:06:25	邓 奇:	1
03:06:27	孙小婷:	1
03:06:28	Administrator:	1
03:06:28	夏敏:	1
03:06:29	李小飞:	1
03:07:07	夏敏:	L2
03:07:10	杨俊桔:	加惩罚
03:07:30	Brian:	后面的惩罚项咋得到的？
03:07:58	Brian:	怎么推导的
03:08:07	Brian:	嗯嗯
03:08:07	Youki:	就是科学家怎么想出来的？
03:08:11	严广宇:	L2损失
03:08:15	匡鸿深:	怎么知道要加一个惩罚项
03:08:27	严广宇:	L1 L2 范数
03:10:41	陈纬铨:	没问题
03:11:00	wuxi:	theta缩放了
03:11:58	Tang JM:	是的
03:11:59	邓 奇:	1
03:12:01	严广宇:	的确
03:12:01	马玄:	1
03:12:02	陈悦:	对
03:12:05	夏敏:	1
03:12:07	李涛:	1
03:12:14	孙小婷:	迭代变慢了？
03:12:33	肖瑶萍:	降低某些神经元的权重？
03:12:37	陈悦:	theta越大减的越多
03:12:37	罗家伟:	相当于权值减小
03:12:51	严广宇:	当梯度过小时，theta也能减小
03:14:30	陈悦:	1
03:14:31	李小飞:	1
03:14:36	Tang JM:	1
03:14:37	龙云淋:	1
03:14:37	陈华涓:	1
03:14:37	陈靖韦:	1
03:14:38	孙小婷:	1
03:14:39	邓 奇:	1
03:14:41	yty:	1
03:15:03	唐陈:	1
03:15:04	王睿:	1
03:15:06	李林洲:	1
03:15:08	夏敏:	1
03:15:29	孟明:	添加惩罚项就是正则化？
03:15:29	yty:	lamda值是怎么确定的
03:16:25	houzhaoding:	加惩罚会不会影响正常结果的输出
03:16:32	李涛:	有没有惩罚过大的情况
03:19:11	luyz:	这个公式 n的累加怎么消失了？
03:19:28	luyz:	惩罚项 n的累加
03:19:47	杜琛萍:	J对thetaj求导
03:19:54	杜琛萍:	就一个
03:20:23	Youki:	正则化是不是可以减小噪声的影响？
03:22:05	陈悦:	1
03:22:07	郑兴纯:	1
03:22:07	孔维昌-cv:	1
03:22:08	邓 奇:	1
03:22:08	龙云淋:	1
03:22:11	王睿:	1
03:23:55	陈纬铨:	过
03:23:56	邓 奇:	1
03:23:57	龙云淋:	可以
03:23:58	李小飞:	1
03:25:12	Tang JM:	山脊回归
03:25:31	杜琛萍:	是套索
03:26:12	包颖:	符号函数
03:28:37	luyz:	1
03:28:38	旭生 孙:	1
03:28:39	严广宇:	1
03:28:39	孟明:	1
03:28:39	tongzongjie:	1
03:28:39	龙云淋:	1
03:28:41	laichengxin:	1
03:28:42	唐陈:	1
03:28:43	李涛:	1
03:30:17	罗家伟:	相切
03:30:19	孙小婷:	1
03:30:24	陈纬铨:	不懂
03:30:27	houzhaoding:	0
03:30:29	Brian:	有点懵
03:30:30	龙云淋:	不懂
03:30:30	李林洲:	1
03:30:31	Tang JM:	屁懂屁懂
03:30:35	王睿:	0
03:31:04	luyz:	1
03:31:04	王睿:	1
03:31:06	严广宇:	1
03:31:06	马玄:	1
03:31:08	龙云淋:	1
03:31:09	孟明:	想起来了
03:31:25	houzhaoding:	平方
03:31:26	马玄:	平方
03:31:29	Baoce SUN:	上周作业？
03:31:30	Brian:	2范数？
03:31:32	Tang JM:	平方相加
03:31:49	包颖:	1
03:31:49	陈纬铨:	1
03:31:51	龙云淋:	可以
03:31:52	马玄:	1
03:31:56	Tang JM:	1
03:32:14	龙云淋:	1
03:32:16	陈纬铨:	1
03:32:17	luyz:	1
03:32:18	韦俊:	1
03:32:20	Tang JM:	1
03:32:20	唐陈:	1
03:32:37	Tang JM:	1
03:32:37	陈纬铨:	1
03:32:39	王睿:	1
03:32:39	luyz:	1
03:32:39	李林洲:	1
03:32:40	龙云淋:	1
03:32:41	严广宇:	1
03:32:41	唐陈:	1
03:32:41	匡鸿深:	1
03:32:46	罗家伟:	拉格朗日算子的概念？
03:32:47	Brian:	切
03:32:53	李林洲:	相交
03:33:12	Tang JM:	1
03:33:13	陈纬铨:	理解
03:33:13	Brian:	1
03:33:14	Tianhu Zhang:	1
03:33:16	龙云淋:	1
03:33:16	李林洲:	1
03:33:17	qinzongding:	111
03:33:18	tongzongjie:	1
03:33:55	luyz:	1
03:33:55	包颖:	1
03:33:57	夏敏:	1
03:34:30	Tang JM:	1
03:34:56	luyz:	1
03:34:58	Tianhu Zhang:	1
03:35:20	tongzongjie:	1
03:36:15	王睿:	为什么L1能选择特征点？老师能不能再讲一下，刚才没听清
03:36:31	王睿:	最后两句
03:36:45	luyz:	对着H（theta）做切线
03:37:21	王睿:	1
03:37:28	高格格:	1
03:37:42	Brian:	没代码吗
03:38:14	Brian:	1
03:38:26	Brian:	剩一半了
03:38:27	严广宇:	哈哈
03:38:29	鄢新义:	1
03:38:31	邓 奇:	1
03:38:39	旭生 孙:	💪🏻💪🏻💪🏻
03:38:47	侠航 陈:	加油
03:38:52	王睿:	加油
03:38:52	Brian:	前面不听后面连不上了
03:38:57	龙云淋:	抠细节
03:38:58	高格格:	💪
03:39:08	laichengxin:	💪💪💪
03:39:17	深瞳:	老师辛苦了
03:39:24	陈靖韦:	机械学习的其他方法要继续学习吗
03:39:26	王睿:	老师辛苦了
03:39:27	yty:	为什么theta可以为零 ，就是L1具有筛选特征值的特点
03:39:28	Administrator:	1
03:39:29	龙云淋:	老师辛苦
03:39:40	杜琛萍:	老师辛苦了
03:40:02	Brian:	决策树什么的还讲吗
03:40:06	Brian:	嗯嗯
03:40:07	李林洲:	导致梯度爆炸的情况是不是很多,上次老师讲到了学习率过大的影响,今天又有激活函数的影响,老师能不能把这方面总结一下
03:40:23	严广宇:	特征工程会讲吗？
03:40:24	高格格:	老师，往后会说一下C++的opencv么？用的时候碰到了一些莫名其妙的error...
03:40:59	旭生 孙:	集成式代码没有看到。。。
03:41:41	yty:	theta为零 ，为什么就是L1具有筛选特征值的特点
03:42:18	Youki:	C++和OpenCV做横向的话，哪个效率高呀？
03:42:31	Youki:	C++和Python做横向的话，哪个效率高呀？
03:42:36	Youki:	对对对。
03:42:38	luyz:	为什么惩罚函数求导以后，n累加没有了，刚才提了一下，还是不明白
03:43:07	yty:	筛选出特征值有什么具体意义吗
03:43:30	yty:	1
03:43:31	houzhaoding:	线性回归的本质是不是都是点距离线的距离最小？只是不同的求解算法，比如最小二乘，梯度下降等。
03:43:40	Administrator:	n是样本量，theta是针对特征来的
03:43:51	Administrator:	所以不用累计
03:43:53	Administrator:	累加
03:44:38	luyz:	理解了
03:44:43	张东青:	最后那一块，为什么theta1，theta2需要同时在椭圆和L1或者L2上呢
03:45:15	张东青:	是
03:45:32	Youki:	正则化隐含了一个原则：就是实际函数更趋于平缓，这不是经验论吗？
03:46:29	高格格:	红色的cost function等值线，越往外cost越大
03:47:19	houzhaoding:	刚才那个问题相交不行吗，得相切
03:47:30	Administrator:	老师，h和r对theta求导同时为0，是吧
03:47:31	houzhaoding:	1
03:47:34	夏敏:	L1 的特点之一是有特征选择的效果，这个效果是使矩阵稀疏化，稀疏的解是训练的目标吗？
03:47:37	Administrator:	就是he
03:47:37	陈靖韦:	那如果需要计算更里面层的cost function呢
03:47:57	Administrator:	就是h和r对theta的导函数同时为0？
03:48:29	Tianhu Zhang:	这周没有作业，可以把这周本来要讲的代码发给大家吗，有时间的就可以自己提前也敲一下
03:48:35	houzhaoding:	老师中文的机器学习书有推荐的吗？好多人看花书
03:49:02	Brian:	西瓜、
03:49:20	陈靖韦:	机械学习实战？
03:49:26	严广宇:	葫芦书
03:49:34	Youki:	或者利用这个经验论继续往后推，就是说存在多个参数，但是我们也不确定，用哪个参数好，所以我们用正则化帮助确定。
03:50:01	houzhaoding:	只是想找本书查查
03:50:23	Brian:	有个西瓜书，清华的那个怎么样？
03:50:39	李涛:	有 中文的
03:50:40	Youki:	我发现一种情况会使正则化失效，你帮忙看看是不是？
03:50:46	Brian:	明白了
03:51:03	Youki:	就是有500个参数，但是只有2个参数有效？
03:51:05	Brian:	是的，把每一个知识点吃透。。。
03:51:49	李小飞:	那有什么技术博客推荐吗
03:51:58	肖瑶萍:	csdn
03:53:03	张东青:	但是有些课堂上的，要完全搞明白吗，比如sift的细节，感觉听课明白，只是知道个意思
03:53:05	深瞳:	老师，那自己想写技术文，你推荐哪个平台呢？最后一个问题
03:53:23	Baoce SUN:	惩罚项是不是跟上周作业有关系？
03:53:36	Baoce SUN:	是的
03:53:42	深瞳:	赵老师再见
03:53:44	严广宇:	博客园、csdn、简书、知乎都可以
03:54:08	罗家伟:	这个感觉就是跟拉格朗日算子的概念相似
03:54:15	houzhaoding:	应该是python 自带的线性回归与惩罚项有关系
03:54:25	孙小婷:	好像是哦
03:54:41	Brian:	上周的作业第一个题和上周课堂讲代码的一样吗？
03:54:54	杜琛萍:	不一样。。
03:54:56	Baoce SUN:	矩阵形式吧？
03:55:01	杜琛萍:	矩阵乘法
03:55:03	luyz:	是不是转化为矩阵运算
03:55:25	houzhaoding:	我以为是直接调用python的库
03:55:50	houzhaoding:	今天作业白做了。哈哈
03:56:16	laichengxin:	老师晚安
03:56:19	高格格:	老师再见
03:56:20	houzhaoding:	老师再见
03:56:20	孙小婷:	88
03:56:20	严广宇:	老师再见
03:56:20	Eric:	这周第一题先不做是吧
03:56:22	邓 奇:	谢谢老师
03:56:22	Baoce SUN:	晚安
03:56:28	Tianhu Zhang:	老师再见
03:56:32	李小飞:	晚安
03:56:32	Eric:	谢谢老师。
03:56:37	夏敏:	晚安
