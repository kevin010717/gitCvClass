00:13:24	陈韵莹:	1
00:13:26	鄢新义:	1
00:13:26	韦俊:	哈罗
00:13:27	chen jinkui:	1
00:13:29	孟乒乒:	·
00:13:31	孟乒乒:	1
00:14:32	张培栋:	1
00:15:12	韦俊:	1
00:15:13	彭冰:	1
00:15:15	潘力:	1
00:15:16	陈悦:	1
00:15:53	邓 奇:	1
00:15:54	徐楠:	 1
00:15:55	潘力:	1
00:15:55	卢宇宙:	1
00:18:58	张磊:	集中答疑
00:19:26	卢宇宙:	呢问题都忘了
00:19:39	Administrator:	那就没有直播的意义了
00:19:48	卢宇宙:	对
00:19:51	彭冰:	其实在过程中 有些问题 会有同学已经回答的很清楚了
00:20:28	蒋开文:	为什么直播不参加，非得要听录播呢
00:20:35	马玄:	没有互动，就没有意义了
00:20:57	Brian:	对的
00:21:24	邓 奇:	同意
00:21:32	孟明:	同意
00:22:42	唐陈:	需要互动
00:25:56	wuxi:	go
00:25:58	王睿:	ok
00:25:58	刘向宏:	0
00:25:59	陈悦:	1
00:26:01	刘权:	1
00:26:01	郑兴纯:	1
00:26:02	卢宇宙:	开始吧
00:26:02	张培栋:	ok
00:26:02	邓 奇:	1
00:26:03	薛毓铨:	继续
00:26:19	Yimin MA:	开车..... :)
00:28:44	张磊:	哈哈
00:28:48	张磊:	漏
00:29:00	张磊:	嘿嘿嘿
00:29:06	Brian:	没事，音对就行了
00:32:16	Brian:	每个人就讲吗？
00:32:20	Brian:	都
00:32:55	小天-king-北京-1年:	就怕做不出来。
00:33:35	张磊:	莫得问题
00:33:39	Brian:	1
00:33:43	小天-king-北京-1年:	试用CNN做，试用什么框架啊？opencv吗？
00:35:08	陈悦:	1
00:35:08	邓 奇:	1
00:35:10	Brian:	1
00:35:10	张磊:	1
00:35:12	徐楠:	服务器是由开课吧提供嘛
00:35:14	Lenovo:	1
00:35:17	潘力:	组队有建议吗？
00:35:58	潘力:	就是自己找队友吗？
00:36:14	小天-king-北京-1年:	我有深度学习主机，北京的同学，可以一起啊。
00:36:49	张培栋:	我也在北京
00:36:59	张培栋:	有一块1080ti
00:37:07	小天-king-北京-1年:	可以一起组队。我的是双1080ti
00:37:23	Eric:	666
00:37:28	蒋开文:	666
00:37:42	蒋开文:	可以分布式计算吗
00:37:53	蒋开文:	大家一起组合一下
00:38:11	龙云淋:	壕。。
00:38:17	小天-king-北京-1年:	图片资源都是上百G的，服务器估计传不上去。
00:38:59	侠航 陈:	训练几十万张图，租个服务器大概要多少钱
00:41:18	张培栋:	mageface人脸数据65G大得很
00:42:24	严广宇:	mask rcnn属于实例分割吧
00:43:53	张培栋:	就是基于faster rcnn，多了一个分割
00:45:08	张培栋:	MTCNN·吗？
00:45:27	李文艺:	mtcnn是人脸检测
00:45:48	张磊:	斜框~
00:45:53	张磊:	我看到了
00:47:49	匡鸿深:	检测跟分割感觉没区别啊
00:48:34	潘力:	老师你的项目可以给我们学习学习吗？
00:48:42	小天-king-北京-1年:	也是用框架做吧？
00:48:51	李文艺:	boundingbox 的转正是怎么做的
00:48:54	小天-king-北京-1年:	例如试用yolo啥的
00:48:58	齐鹏飞:	Gan什么时候讲呀？
00:49:17	夏敏:	带有不规则的GT框是用什么方法产生的？
00:49:26	齐鹏飞:	1
00:49:29	李文艺:	就是右上角的，是转歪
00:49:49	彭冰:	我也对GAN感兴趣 哈哈
00:49:57	小天-king-北京-1年:	做这个项目，用什么开源框架啊？
00:58:06	匡鸿深:	既然不知道原理，为啥还能创建出来
00:58:17	齐鹏飞:	实验出真知
00:58:35	刘权:	反正工科，能用就行
00:59:00	k:	我们复现cnn吗
01:01:06	Eric:	数据增广最好的方法是GAN吗？
01:02:45	齐鹏飞:	普通增广就行了
01:03:21	齐鹏飞:	GAN一般用来以假乱真的，但一般只能是已有的
01:04:01	小天-king-北京-1年:	是学习的图片非常少的时候才需要增广吗？
01:04:01	彭冰:	嘿嘿
01:04:35	李文艺:	图片少，是不是gan效果不好啊
01:04:38	Tang JM:	图片少就不能用深度学习了吧
01:04:47	李文艺:	那该怎么训练gan呢
01:05:07	Xie ChuYi:	感觉Optimization和 Loss Update Strategy, Learning Strategy都差不多是一个意思吧？分不清。。
01:05:08	何川:	selection strategy，是说kfold吗？
01:06:15	齐鹏飞:	1
01:06:18	邓 奇:	1
01:06:20	潘力:	1
01:06:24	k:	好
01:06:24	李文艺:	图片少，是不是gan效果不好啊，大概300左右
01:06:26	Yimin MA:	CNN针对的场景举得例子都是是对图片的，那对视频流是怎么整呢？
01:06:34	WW BB:	9:00开始
01:06:38	齐鹏飞:	接真把
01:06:48	齐鹏飞:	截帧
01:07:08	wuxi:	语音呢？
01:07:12	齐鹏飞:	视频是不是要用到lstm
01:07:12	Eric:	one-stage比two-stage准确率低是必然的吗？
01:07:14	小天-king-北京-1年:	我感觉视频就是图片的集合，是把声音也加上了？
01:07:42	夏楠:	视频  图像加一个时空维度而已。
01:07:48	Eric:	没有听清楚，刚刚one-stage那个。
01:08:01	齐鹏飞:	视频是不是用到lstm
01:08:24	齐鹏飞:	1
01:08:46	罗家伟:	游戏AI涉及的是不是强化学习？
01:09:00	陈韵莹:	Cnn可以用于OCR吗？
01:09:09	匡鸿深:	老师，既然cnn是不可解释的，那些写框架的人是怎么实现的
01:09:13	齐鹏飞:	ocr一般是cnn+rnn
01:09:22	张培栋:	OCR属于CV
01:09:32	Eric:	我觉得CNN是可以解释的。
01:09:44	潘力:	可能是调出来的吧
01:09:46	齐鹏飞:	有研究可解释的cnn
01:09:50	齐鹏飞:	但效果不好
01:09:58	齐鹏飞:	就是没有负反馈的前向
01:10:08	齐鹏飞:	人工设计的
01:11:56	薛毓铨:	DNN是主要是用来解决哪方面问题的...跟CNN有啥区别
01:12:27	齐鹏飞:	我理解就是所有深度学习网络的统称吧。。。
01:12:50	Xie ChuYi:	想知道CNN是怎么处理视频的，还有三维卷积是啥
01:13:01	齐鹏飞:	嗯，我也期待
01:13:24	齐鹏飞:	3d可能就是加了上下文？
01:13:34	韦俊:	1
01:13:34	邓 奇:	1
01:13:34	潘力:	1
01:13:35	张培栋:	1
01:13:35	齐鹏飞:	1
01:13:36	彭冰:	1
01:13:36	唐陈:	1
01:13:37	薛毓铨:	1
01:13:37	严广宇:	1
01:13:37	夏敏:	1
01:13:38	孔维昌-cv:	1
01:13:39	徐楠:	1
01:13:39	卢宇宙:	1
01:14:54	夏楠:	接触到的 处理视频主流方法 一种3D卷积 一种2stream
01:17:55	卢宇宙:	0
01:18:04	马玄:	0
01:19:34	齐鹏飞:	1
01:19:35	马玄:	1
01:19:35	韦俊:	1
01:19:35	Tang JM:	1
01:20:16	罗家伟:	每个kernel是有3个嘛
01:20:38	罗家伟:	都一样嘛
01:20:46	小天-king-北京-1年:	不同的kernel提取不同的特征吗？
01:20:58	罗家伟:	1
01:21:00	Eric:	三个channel出来地点值是直接相加吗？
01:21:17	齐鹏飞:	应该是
01:21:17	Xie ChuYi:	在这里的每个kernel的channel和input image的channel一样
01:21:23	Tang JM:	kernel 应该就是之前学得filter
01:21:31	Xie ChuYi:	是的
01:22:01	刘向宏:	1
01:22:02	严广宇:	1
01:22:02	齐鹏飞:	1
01:22:04	马玄:	1
01:22:05	卢宇宙:	1
01:22:40	李文艺:	kernel size
01:22:48	yangfanqihang:	kernel 如何决定颜色的？
01:23:10	何川:	padding
01:23:11	刘向宏:	看padding？
01:23:16	彭冰:	取决于padding
01:23:21	夏敏:	还有stride
01:23:21	qinzongding:	跟kernel有关
01:23:22	李林洲:	和以前卷几层一样
01:23:23	Tang JM:	w 和 h 和padding 和 步长有关
01:23:25	严广宇:	H = （H_in - k-1/2 + 2 * pad）/ stride
01:23:26	Xie ChuYi:	通过padding和stride
01:23:33	侠航 陈:	根padding有关，还有跳的步数
01:24:00	严广宇:	H =1 +（H_in - k-1/2 + 2 * pad）/ stride
01:25:19	严广宇:	写错了，不改除2的……
01:25:55	何川:	dilation有什么作用啊？
01:26:01	qinzongding:	stride是什么？
01:26:05	qinzongding:	没看明白
01:26:15	卢宇宙:	解释一波
01:26:17	何川:	stride是一次走多少格
01:26:34	齐鹏飞:	下采样
01:27:02	邓 奇:	不同的kernel提取不同的特征吗？为什么有这么多kernel呀
01:27:27	Lenovo:	1
01:27:30	夏敏:	1
01:27:39	齐鹏飞:	1
01:27:47	Brian:	11
01:27:50	卢宇宙:	1
01:30:27	陈悦:	1
01:30:28	林汉斌:	1
01:30:28	侠航 陈:	1
01:32:36	小天-king-北京-1年:	W00的排列为什么不是方块的？
01:35:35	k:	为什么loss既对w求导也要对 I 求导
01:35:38	徐楠:	1
01:35:39	彭冰:	1
01:35:40	邓 奇:	1
01:36:28	李林洲:	为了向前一层继续传播
01:36:48	蒋开文:	这个式子没看懂啊
01:40:28	彭冰:	I的系数其实都是1吧？
01:43:22	齐鹏飞:	厉害了
01:43:48	齐鹏飞:	反卷积吗
01:45:37	齐鹏飞:	1
01:46:16	匡鸿深:	实操一下怎么做的
01:46:54	Brian:	就理解成可逆的就行了吧
01:48:05	Tang JM:	AutoEncode 和这个有什么关系
01:49:02	齐鹏飞:	这就是dilation?
01:52:11	严广宇:	1
01:52:13	邓 奇:	1
01:54:37	严广宇:	1
01:54:47	何川:	img2col实际用得多部
01:55:01	小天-king-北京-1年:	我得课后再看，才能理解
01:55:20	k:	有没有这些推到的代码
01:55:25	k:	推导
01:55:37	匡鸿深:	im2col的矩阵是怎么变换得到
01:55:44	何川:	刚才deconv，transpose conv 和 partial 。。 是一个概念？
01:55:53	何川:	有点混了
01:57:14	Brian:	就是把filter放大了？
01:58:22	唐陈:	脑壳痛
01:58:35	潘力:	666
01:58:52	何川:	2
01:58:56	刘权:	3
01:58:59	马玄:	3
01:59:03	Tang JM:	4
01:59:03	李晶:	4
01:59:05	何川:	4
01:59:13	何川:	4 ---> 2
01:59:15	刘权:	不算channel就是3
01:59:30	齐鹏飞:	n
01:59:37	夏楠:	4帧
01:59:46	齐鹏飞:	number of image
01:59:51	何川:	1
02:00:07	qinzongding:	4帧 怎么理解？
02:00:14	徐楠:	4张图片
02:00:55	彭冰:	1
02:01:25	卢宇宙:	1
02:01:26	齐鹏飞:	可不可以是w*h*12
02:02:16	陈靖韦:	没有第四张图？
02:02:16	卢宇宙:	1
02:02:16	邓 奇:	1
02:02:17	高格格:	1
02:02:18	何川:	这样卷的结果是640*480*k*4?
02:02:18	罗家伟:	1
02:02:19	qinzongding:	不太明白
02:02:19	李晶:	1
02:02:21	刘向宏:	1
02:02:27	罗家伟:	4帧变成2帧？
02:02:29	Tang JM:	1
02:03:03	罗家伟:	哦哦，相当于在时间维度做了个kernel
02:03:44	齐鹏飞:	1
02:03:44	郑兴纯:	1
02:03:47	卢宇宙:	1
02:03:50	潘力:	能懂
02:04:36	卢宇宙:	1
02:04:36	Tianhu Zhang:	1
02:04:36	郑兴纯:	1
02:04:37	潘力:	1
02:04:39	Yimin MA:	1
02:04:39	徐楠:	1
02:04:43	罗家伟:	帧数变低会有影响吗
02:04:47	齐鹏飞:	batch多大比较好呢
02:05:10	李文艺:	3d卷积可以实时吗？
02:06:05	唐陈:	明天？
02:06:06	Tang JM:	明天有课？
02:07:36	李文艺:	3d卷积可以实时吗？
02:07:43	邓 奇:	1 刚才3个红实线卷出来结果是相加得到一张图吗？2 不同的kernel提取不同的特征吗？为什么有这么多kernel呀
02:08:00	马玄:	kornel是3帧怎么理解？是随便取的吗？
02:08:46	陈靖韦:	刚刚是三帧图片卷积成一帧了吗
02:09:13	李林洲:	高阶的卷积(非普通的卷积)实际用的多吗
02:09:41	Eric:	三个channel的点是直接相加吗？
02:10:14	邓 奇:	1
02:10:15	潘力:	好
02:10:20	WW BB:	8:02
02:10:25	Tang JM:	10点
02:10:30	WW BB:	10:02
02:10:50	Eric:	休息的时候能再讲一下BP那里吗？
02:11:54	k:	老师，刚刚讲的理论部分有没有代码，理论太难了，
02:12:35	齐鹏飞:	没太懂求I的导是哪里用到传播到前一层的，不是就改变权重的大小吗
02:14:23	陈靖韦:	bp那一块是不是讲得我们对函数进行卷积后能对图片进行还原
02:15:34	潘力:	https://blog.csdn.net/bitcarmanlee/article/details/78819025
02:15:34	严广宇:	1
02:15:35	彭冰:	1
02:15:36	邓 奇:	1
02:15:37	刘向宏:	1
02:15:38	xuanye19920202:	1
02:15:38	夏敏:	1
02:15:40	潘力:	1
02:15:40	孔维昌-cv:	1
02:15:59	qinzongding:	刚才老师说花了两个小时画的那个图那里 能再讲一下吗
02:18:42	严广宇:	没有运算
02:18:45	小天-king-北京-1年:	结果只有0和1
02:18:46	龙云淋:	梯度
02:18:49	彭冰:	朝着x和y的方向的梯度
02:18:52	刘向宏:	梯度传递
02:18:53	Tianhu Zhang:	不会梯度消失
02:18:58	马玄:	梯度不爆炸
02:24:01	蒋开文:	有半轴还是一个线性函数，relu怎么实现的非线性？
02:24:24	蒋开文:	右
02:25:13	林汉斌:	那就线性看
02:25:26	蒋开文:	ok
02:25:27	齐鹏飞:	有些场景还是用sigmod，是因为relu丢到了一半信息吗
02:26:21	刘权:	1
02:26:23	蒋开文:	1
02:29:13	林汉斌:	跟卷积差不多啊
02:29:27	齐鹏飞:	pooling不用参数
02:29:53	小天-king-北京-1年:	感觉池化就是在卷积啊。是没有卷积核吧？
02:30:42	齐鹏飞:	池化也是为了增大感受野吗
02:31:23	夏敏:	average pooling是不是很少用啊？
02:31:31	齐鹏飞:	global pooling
02:31:53	马玄:	那些数字是什么意思？
02:32:14	蒋开文:	size与dimension区别？
02:33:14	林汉斌:	懂了
02:33:18	小天-king-北京-1年:	减少dimension是rgb变灰度图了吗？
02:34:26	林汉斌:	BP看一下
02:34:32	qinzongding:	只有Max 和average Pooling 吗
02:35:00	彭冰:	1
02:35:02	qinzongding:	111
02:37:05	卢宇宙:	加权吗？
02:37:39	齐鹏飞:	拉直
02:37:54	刘权:	打成1维
02:38:28	罗家伟:	所以每个点是一个像素点？
02:38:35	Tang JM:	为什么拉长1维
02:38:40	潘力:	为什么这么操作呀
02:38:43	齐鹏飞:	特征点？
02:39:13	卢宇宙:	flatten 过程要卷积吗？
02:39:22	侠航 陈:	整个过程感觉跟sift有点像
02:39:28	齐鹏飞:	维度为1*1的feature map
02:40:42	齐鹏飞:	加两层
02:40:47	罗家伟:	所以在flatten的时候回选取特征点？
02:40:47	齐鹏飞:	是为了什么
02:41:15	Bruce:	两层Relus？为什么？
02:41:24	李林洲:	一般网络最后需要几个全连接层?
02:41:29	齐鹏飞:	两层fc是为了多加些参数判断的更准吗
02:41:35	郑兴纯:	1
02:42:39	何川:	不用pooling层用什么替代啊？
02:42:45	Tang JM:	是啊，为啥要用全连接层
02:42:58	Baoce SUN:	BP感觉很复杂
02:43:00	何川:	不用fullyconnectedlayer层用什么替代啊？
02:43:18	Brian:	overfitting?
02:43:35	齐鹏飞:	dropout?
02:43:36	Tang JM:	不用，拉直后直接送到sigmoid、或者 softmax
02:45:58	李文艺:	dropout 
02:46:05	李文艺:	batch norm
02:49:31	齐鹏飞:	都用
02:50:46	马玄:	0
02:50:49	罗家伟:	0
02:50:53	陈靖韦:	0
02:50:54	Baoce SUN:	0
02:50:56	潘力:	回头看看，应该可以理解
02:50:58	马玄:	为什么乘以p
02:51:09	qinzongding:	Fc1 是什么？
02:51:13	陈靖韦:	期望吗
02:51:14	齐鹏飞:	p就是一个概率值吧
02:51:16	潘力:	p是个概率
02:51:54	马玄:	Train和test是一个p吗？
02:52:02	陈靖韦:	是吧
02:52:03	卢宇宙:	对
02:52:36	k:	fc1是啥
02:53:04	Eric:	感觉这个就是乱搞。
02:53:18	严广宇:	Dropout一般放在哪些层后面？
02:53:23	李林洲:	dropout已经申请专利了?
02:53:48	何川:	除以p对bp过程有影响不？
02:53:48	齐鹏飞:	训练的时候是每个Batch都随机dropout吧
02:54:09	Eric:	现在应该已经用的不多了吧，这个dropout？
02:54:29	齐鹏飞:	1
02:55:08	Eric:	批正则化。
02:59:29	齐鹏飞:	有relu了为啥还要Bn
03:00:07	卢宇宙:	0
03:00:40	Eric:	同问，不是有了relu吗？
03:00:55	蒋开文:	中间层不是不用sig
03:01:25	李文艺:	什么原因导致的shift呢
03:04:40	齐鹏飞:	变回去？
03:05:35	罗家伟:	费劲
03:05:50	李晶:	BN在RELU之前还是之后呢？
03:06:08	Eric:	yi是什么意思？
03:06:08	齐鹏飞:	过完sigmod再变回来？
03:07:58	齐鹏飞:	拉回来过sigmod还是过完sigmod再拉回来
03:08:07	卢宇宙:	拉回来以后再重新训练一次？
03:08:15	卢宇宙:	还是增加参数
03:08:30	匡鸿深:	相当于训练同时考虑权重和分布位置么
03:08:41	Eric:	放在激活函数之前吗？
03:08:46	齐鹏飞:	如果是sigmod之前就拉回来不是相当于什么都没干吗
03:09:05	林汉斌:	gganma beta是我们设置的，还是学习来的
03:10:10	wuxi:	同问，这里gama，beta不是参数变多了吗？
03:11:52	潘力:	把数据拉过去训练，拉回来验证？是这个意思吗
03:12:07	林汉斌:	它怎么学习啊，学习策略是啊
03:12:20	Eric:	scale 和shift在relu之前吗？
03:12:37	陈靖韦:	拉过去之后学习，然后通过学习拉回来是吗
03:12:41	李文艺:	bn + relu 也是同样解释吗？
03:13:37	蒋开文:	拉来拉去数据不是一样吗？
03:13:46	齐鹏飞:	训练完再拉回来？
03:14:03	齐鹏飞:	如果是sigmod之前就拉回来不是相当于什么都没干吗
03:14:16	Eric:	我也感觉没有用。
03:14:40	齐鹏飞:	一直没有讲呀。。。
03:14:44	卢宇宙:	训练数据拉完以后在激活函数位置不同了
03:15:11	齐鹏飞:	要同分布的话不就是恢复原来的位置吗，其他同学理解了吗
03:15:27	齐鹏飞:	能解释下吗，有理解的同学
03:16:55	林汉斌:	数据分布一样，但数据跟原来的不一样
03:17:33	潘力:	BP的时候，rule折点不可导，会不会产生问题？
03:18:45	齐鹏飞:	@林汉斌，怎么理解你这句话呢。。
03:20:10	林汉斌:	比如我原来的数据服从正太分布，BN后的数据也服从正太分布，但数值变了，
03:20:30	齐鹏飞:	但是他有拉回去了呀
03:21:16	齐鹏飞:	你是说程度不一样吗
03:22:23	林汉斌:	可以这样理解吧，比如高斯分布有很多形式是吧
03:23:17	齐鹏飞:	感觉挺玄乎的
03:23:39	齐鹏飞:	还是不可解释吧，就把这种程度丢给网络去学习
03:23:59	李林洲:	之所以最后加上u和b,是保障模型有能力将数据还原为以前的分布
03:24:07	Bruce:	等会儿再讨论，聊天框弹来弹去的。
03:24:15	林汉斌:	老师现在不是在说怎么了吗
03:24:23	李林洲:	最终要不要还原,是系统学习过程中自己决定的
03:25:25	齐鹏飞:	嗯，让网络自己决定梯度的程度
03:25:42	林汉斌:	是的
03:29:58	齐鹏飞:	lrn
03:30:50	Tang JM:	nice
03:31:05	赖盛鑫:	nice
03:32:22	Eric:	这周没有作业吗？
03:32:32	齐鹏飞:	@老师，relu的梯度为1，bn的作用更多体现在哪里呢？
03:32:32	卢宇宙:	没作业太爽了
03:32:36	k:	老师，后头给些今天晚上理论推导的实例代码吧，今晚理论沦陷了
03:32:53	Eric:	Conv的BP请再讲一遍。
03:33:03	彭冰:	今天其实不是理论沦陷 是东西多 太绕了
03:33:12	小天-king-北京-1年:	之前的课程是面试需要，CNN开始是工作需要把？
03:34:42	李文艺:	视频用C3D多吗，还是把视频分成图片做？
03:35:10	高格格:	老师，那个trello上传的pdf好像和课上用的不太一样，没有动图，之后要更新吗OvO?
03:35:11	夏楠:	C3D也是图片来的。
03:35:46	齐鹏飞:	@老师，每一个conv层后都会有一个relu吗？
03:35:54	高格格:	好~
03:36:12	李文艺:	relu使用bn的好处和sigmoid一样吗？
03:36:23	Yimin MA:	有没有基于一个CNN project解剖一下各层，结合代码，好理解些。今天搞得CNN也不比之前的简单，心慌慌。
03:36:42	齐鹏飞:	@老师，relu的梯度为1，bn的作用更多体现在哪里呢？
03:36:44	k:	对，加上些代码
03:36:58	k:	太抽象了
03:37:36	卢宇宙:	伪代码就行
03:37:40	卢宇宙:	帮助理解的
03:37:48	k:	可以
03:38:08	潘力:	relu折点处不可导，会不会有问题呀？
03:38:10	Yimin MA:	是不是实现细节都在框架里实现，剩下CNN的工程其实就是一个调优的过程？
03:38:36	Eric:	Conv的BP怎么算的，没有听懂。
03:39:48	齐鹏飞:	@老师，不好意思最后再问一下，relu的梯度为1，bn的作用更多体现在哪里呢？
03:39:50	Yimin MA:	嘿嘿，今天这节课完了，自己搞一个层，好像有点做梦的感觉。
03:39:55	李林洲:	下了,老师辛苦了
03:40:50	齐鹏飞:	ok
03:41:44	夏楠:	老师，请问把BN 理解为 不同训练数据会有不同的分布情况，为了便于激活函数提取数据分布特征，所以需要把数据映射到 激活函数的有效区间， 然后提取特征之后 再把数据映射回原来的分布情况 从而继续后续的训练  可以吗？
03:43:38	潘力:	好的，谢谢老师，各位老铁晚安咯！明天看回放，溜了。
03:43:39	夏楠:	哦哦，知道了。谢谢老师。
03:43:43	赖盛鑫:	谢谢老师
03:44:13	高格格:	老师再见
03:44:16	郑兴纯:	下课
03:44:18	孔维昌-cv:	谢谢老师，老师晚安
03:44:25	刘向宏:	谢谢老师
03:44:33	Brian:	看课纲大项目是在9周之后吗？
03:46:03	陈靖韦:	老师，在卷积层的时候，三帧图片与三帧karnel，怎么合成一帧图片
03:46:46	孟明:	老师，实际应用反卷积的时候，是要先在像素之间填零再做卷积吗？
03:46:51	陈靖韦:	好的
03:48:00	Eric:	每层都要pooling，最后一个Tensor的维数不会太小吗？
03:48:27	齐鹏飞:	老师，c3d中把时间多做了一唯，那能不能将多张图片的channel融合保持原有维度不变。这样可不可以保持相同的效果
03:48:33	孟明:	谢谢老师
03:49:09	齐鹏飞:	w*h*3*4 => w*h*12
03:49:30	齐鹏飞:	如上
03:50:30	齐鹏飞:	效果一样吗
03:50:38	陈靖韦:	那就缩短成一个kernel了吗
03:50:44	齐鹏飞:	时间维度的attention？
03:50:49	Eric:	肯定不一样啊。
03:51:29	Eric:	你这相当于12个通道，
03:51:52	Eric:	时间上的关联不知道怎么体现。
03:51:53	齐鹏飞:	谢谢老师啦
03:51:59	齐鹏飞:	晚安
03:52:03	陈靖韦:	谢谢老师
03:52:35	Brian:	溜了晚安
03:52:40	Eric:	CNN相当于降维吗？
