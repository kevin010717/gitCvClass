00:09:59	徐楠:	1
00:10:00	斯太康:	YES
00:10:00	邓 奇:	可以了
00:10:00	刘权:	ok
00:10:04	张磊:	1
00:10:12	潘力:	1
00:10:17	王睿:	1
00:10:30	张磊:	开个小会
00:11:02	张磊:	没问题老师
00:11:10	张磊:	放心走吧~
00:11:29	肖瑶萍:	为什么没有声音
00:11:37	王睿:	有
00:11:38	高格格:	1
00:11:38	邓 奇:	有
00:11:39	孙小婷:	1
00:11:40	徐楠:	可以的
00:11:41	侠航 陈:	1
00:11:41	张磊:	1
00:11:45	卢宇宙:	1
00:11:46	潘力:	有声音
00:17:14	k:	这里的抽象是什么意思啊
00:17:37	邓 奇:	越来越小。。。
00:17:43	张磊:	抽象的特征可以可视化么？
00:19:24	严广宇:	需要进行处理才能可视化。有目前有网络可视化这方面的研究
00:21:09	严广宇:	https://blog.csdn.net/dcrmg/article/details/81255498
00:21:23	周宏雷:	https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
00:23:09	张磊:	好的，多谢
00:23:18	肖瑶萍:	多谢分享
00:24:26	肖瑶萍:	老师那GAN什么时候讲
00:24:42	王睿:	选修
00:25:29	Tang JM:	听成GNN了
00:25:50	侠航 陈:	不错的资料，多谢分享
00:26:15	Song:	什么资料呀，能不能再发一下，刚刚才进来，
00:26:24	肖瑶萍:	https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
00:26:35	肖瑶萍:	https://blog.csdn.net/dcrmg/article/details/81255498
00:26:54	Song:	好的，谢谢！
00:28:30	张磊:	哇~
00:28:37	张磊:	做好心理准备
00:33:02	Song:	我怎么感觉以前的代码就是用的全0呀？
00:34:36	Song:	blob是什么？
00:34:45	唐陈:	block
00:34:57	Song:	哦哦，谢谢。
00:35:14	唐陈:	真的是blob
00:40:25	高格格:	好
00:40:55	Song:	就是对好的初始化，带来了什么增益，还是不太清楚？
00:41:31	Song:	是说Weights有恒定的标准差吗？
00:43:14	k:	主成分
00:44:53	Song:	不是很懂，
00:45:01	Song:	解决了什么问题？
00:45:07	邓 奇:	百化啥意思？
00:45:12	林汉斌:	为什么还要白化
00:45:55	陈悦:	标注化 方差为1 的时候不是圆吗？
00:45:56	侯兆鼎:	处理的目的是啥
00:46:03	陈智杰:	算归一化吗
00:46:20	严广宇:	测试阶段的时候，也需要进行PCA和白化？
00:46:29	李晶:	为什么normalization不能变成圆？
00:46:43	彭冰:	老师 这里处理的数据是什么的数据 处理的目的是什么啊
00:46:55	Tang JM:	特征分解？
00:47:14	夏敏:	降维
00:47:17	马玄:	降维
00:49:56	马玄:	保持信息量
00:49:57	侯兆鼎:	一张图片这个方向有几个
00:50:04	Xie ChuYi:	数据信息损失更小
00:50:52	陈靖韦:	熵
00:53:38	刘权:	1
00:53:39	张天虎:	1
00:53:39	邓 奇:	1
00:53:40	龙云淋:	可以
00:53:40	陈智杰:	1
00:53:40	卢宇宙:	1
00:53:41	徐楠:	明白
00:53:42	陈靖韦:	1
00:53:42	孟乒乒:	1
00:53:43	陈悦:	1
00:53:44	吴彬:	1
00:53:46	Song:	但是感觉还是会丢失信息，不是吗？
00:54:16	Xie ChuYi:	肯定会丢失信息
00:54:30	Song:	哦哦，
00:54:45	陈靖韦:	毕竟降维
00:54:50	龙云淋:	保留主要（本质）特征就好
00:54:59	Song:	哦哦，谢谢，
00:56:05	陈靖韦:	降至2维？
00:56:16	Song:	第一个轴就是第一个正交基，对吧？
00:58:00	Song:	轴跟正交基有关系吗？
00:58:25	Song:	喔喔，每个轴都是相互正交的，对吧？
00:58:30	蒋开文:	二维的数据是不是只有一个主方向
00:59:43	潘力:	怎么找主方向轴呢？二维的话是通过旋转吗？
00:59:56	威 邵:	特征向量内的每个向量是相互正交的？
01:00:14	Xie ChuYi:	笛卡尔坐标系是一个三维正交基（1，0，0）（0，1，0）（0，0，1）这样理解没错吧？
01:07:13	蒋开文:	拉格朗日
01:09:13	罗家伟:	协方差的特征值
01:09:19	严广宇:	特征值？
01:11:27	侯兆鼎:	这个协方差是谁的？
01:11:31	卢宇宙:	阿尔法*阿尔法T为啥是1
01:11:40	马玄:	协方差怎么表示？
01:11:57	卢宇宙:	ok
01:12:57	Song:	怎么感觉跟特征值的公式不太一样，特征值公式的左边不是Ax吗？
01:14:33	Song:	就是说v的方差与lambda相等？
01:14:38	马玄:	导数的推导不太详细？
01:15:10	Song:	这部分推导有手写板的资料吗？
01:15:51	Song:	好的，谢谢，
01:16:02	Song:	这里的U是什么？
01:16:04	陈靖韦:	svd是啥
01:16:17	龙云淋:	奇异值分解
01:16:23	龙云淋:	矩阵论知识
01:16:27	Lenovo:	奇异值分解
01:16:37	Song:	8分钟里面可以提问吗？
01:16:48	Song:	这里的U是什么？
01:16:52	WW BB:	9:15回来
01:17:27	何川:	PCA怎么用在图像处理上？
01:18:11	包颖:	协方差矩阵就是特征值公式里的A啊。  不是v的方差与lambda相等，是求v方差最大，也就是等价于求lambda最大@song
01:18:24	包颖:	U是奇异值分解出的U
01:18:51	潘力:	各位有将这部分的资料链接吗？
01:18:59	潘力:	讲
01:19:08	龙云淋:	U就是标准正交基
01:19:11	包颖:	cov=USV*
01:20:27	龙云淋:	准确来说，是U的列向量构成了我们需要的那组标准正交基
01:21:04	Song:	好的，谢谢大家，我仔细再想想，
01:21:12	Song:	线性代数的知识有点不记得了，
01:22:41	卢宇宙:	三维数据应该怎么处理？
01:26:22	严广宇:	1
01:26:47	WW BB:	AAM
01:27:20	Song:	现在PCA用的还多吗？
01:28:20	Song:	CNN里面会用PCA吗？
01:29:22	Tang JM:	买
01:30:24	Song:	之前听到了一种说法，说所有的训练结果都取决于原始数据，增广的数据增益不大，这种说法，对吗？
01:31:25	严广宇:	你原始数据足够丰富和多的时候，数据增强可以不用。不过现实项目中，没有那么多数据，也不够丰富……
01:31:41	Song:	喔喔，那增广还是有用的，
01:33:00	邓 奇:	GD和BGD是一个意思吗？
01:33:59	Song:	第二行红字是SGD吗，怎么看起来像AGD?
01:34:25	Xie ChuYi:	SGD
01:34:39	Song:	哦哦，谢谢，
01:36:59	Song:	不应该是质量吗？
01:41:06	Song:	就是跟之前的梯度也有关联了，对吗？
01:41:31	陈靖韦:	v是轴？
01:41:33	Tang JM:	那v0 是啥
01:42:02	马玄:	v是迭代变量
01:42:22	蒋开文:	这个有点像模拟退火算法
01:44:18	陈靖韦:	那会梯度爆炸吗
01:44:41	蒋开文:	V是什么
01:44:51	Song:	不是有自动改变学习率的方法吗？
01:44:56	qinzongding:	会不会找不到最小值？
01:44:57	侯兆鼎:	惯性冲？为啥冲成了那个样子
01:48:00	Song:	有不有可能Zig-zag更严重了？
01:48:23	Song:	惯性越大，不是冲的越远吗？
01:54:46	卢宇宙:	求导过程更复杂
01:54:46	罗家伟:	计算量太大？
01:55:19	Song:	每次的输入都不一样，
01:55:23	陈靖韦:	把全部都递归了一遍吗
01:55:24	Song:	都要算一下偏导数。
01:55:32	马玄:	求二阶导
01:55:38	Song:	次次都算偏导，要疯了，
01:56:11	卢宇宙:	LOSS是基于theta的函数
01:56:18	卢宇宙:	不是基于这个式子的函数
01:56:38	潘力:	链式法则，使得求导爆炸
01:59:38	Song:	直接用theta_(i-1)不就行了？
02:00:01	侯兆鼎:	退更新啥意思
02:00:04	马玄:	加一个变量保存原来的theta不就可以了
02:00:07	Song:	为什么要undo呢？
02:00:39	Song:	直接用theta_(i-1)算梯度不就行了吗？
02:03:25	Song:	我觉得本来就应该是三步。
02:03:32	Song:	这个五步没有看懂，
02:05:56	潘力:	这个会不会跟正则项产生冲突？
02:12:41	Song:	这种改进真的效果好吗？
02:12:58	Song:	就是避免了一些局部极小值吗？
02:15:14	Song:	说的是学习率自适应吗？
02:15:40	Song:	哦哦，
02:16:21	Xun Li (Mary):	和normalization有啥关系吗
02:17:10	Song:	如何评价这些方法好不好呢？
02:20:10	潘力:	好
02:20:11	邓 奇:	1
02:20:11	qinzongding:	好
02:20:12	卢宇宙:	1
02:20:13	陈靖韦:	1
02:20:13	孟乒乒:	1
02:20:24	唐陈:	1
02:20:25	Eric:	可以问问题吗？
02:20:29	Tang JM:	太硬了，要消化一下
02:20:35	徐楠:	10分钟吧
02:20:45	WW BB:	10:20回来
02:20:47	Eric:	怎么评价一个优化方法的好坏？
02:21:02	WW BB:	看最终结果好坏。。。
02:21:10	侯兆鼎:	老师， 今天的这些东西都需要代码实现对吗                                                                                                         
02:21:25	WW BB:	代码早就有啦
02:21:30	WW BB:	都是函数直接调用
02:21:33	WW BB:	如果写代码
02:21:43	WW BB:	直接写就行。。。就跟数学公式一样的
02:22:24	侯兆鼎:	那就是理解他的来龙去脉，以及怎么用。这样理解对吗
02:26:24	Song:	也就是说，用了这些方法之后，得到的结果的loss会更低，对吧？
02:27:27	卢宇宙:	应该是训练的更快吧
02:27:36	Song:	喔喔，这样子，
02:27:39	卢宇宙:	最终LOSS都是差不多的吧
02:27:43	卢宇宙:	我是猜测的
02:27:44	Song:	喔喔，
02:27:54	卢宇宙:	等一下等老师回答确认啊
02:27:55	Song:	也就是迭代的次数会少些吗？
02:28:16	Song:	好，讨论一下更加清晰，
02:29:46	潘力:	1
02:29:46	邓 奇:	1
02:30:09	Song:	也就是迭代的次数会少些吗？
02:30:29	Song:	哦哦，
02:32:50	qinzongding:	根号里面的是那一串？
02:33:00	Song:	肯定有啊，
02:33:12	Song:	要是有的梯度就是不同维度不一样呢？
02:33:48	何川:	下面的求和是从第一次开始累积求和吗？？
02:34:06	徐楠:	根号下万一小于0呢
02:34:23	何川:	平方啊……
02:36:39	蒋开文:	这个方法比上一个方法好吗
02:41:40	严广宇:	好像是李飞飞实验室的？
02:42:18	Song:	赵老师也厉害呀
02:59:35	Tang JM:	TP/(TP+FP)
03:00:02	Tang JM:	F1
03:02:27	Song:	人脸很多，
03:02:28	Tang JM:	系统更安全
03:02:33	Song:	 不漏，
03:04:19	陈靖韦:	猜的准？
03:05:02	Song:	不会把小猫当人
03:16:42	罗家伟:	会讲一个类似项目的代码讲解吗
03:16:49	Song:	老师一般是怎么选择刚刚那些梯度的方法的？
03:17:12	Song:	哦哦，
03:17:33	马玄:	lr rate的更新有什么建议？
03:17:38	陈靖韦:	明天再看一遍。。
03:17:42	侠航 陈:	要好好消化下
03:17:43	Song:	学习率您一般是怎么选的？
03:18:00	孟乒乒:	老师问一个其它的，好多视觉招聘要求熟悉opencv，这个是不是熟悉一些常用的api就可以了？
03:18:16	Song:	Adam不需要选学习率吗？
03:18:59	Song:	哦哦，这样呀，
03:19:04	Song:	厉害了。
03:19:25	潘力:	好的，谢谢老师
03:19:26	马玄:	老师辛苦
03:19:28	邓 奇:	谢谢老师
03:19:33	Song:	下周前您会回来吗？
03:19:33	潘力:	再见
03:19:36	孟乒乒:	辛苦
03:19:36	陈悦:	拜拜
03:19:45	高格格:	老师再见
