00:17:41	孟明:	老师，yolo,ssd这些会讲吗
00:19:29	Eric:	YOLO是不是比SSD好一些？
00:19:46	王睿:	GAN第几周开始讲呢？
00:20:53	Eric:	厉害了！
00:21:04	唐陈:	掌声送给他
00:21:19	张磊:	老师，代码会讲么？
00:21:24	Eric:	上节课关于什么的推导？
00:25:30	王睿:	网络优化方法的数学推导
00:26:11	Eric:	哦哦，谢谢，
00:29:18	Eric:	为什么network是三角形？
00:29:49	Eric:	哦哦，谢谢。
00:32:21	何川:	二分类最后vector 是 batches*1还是batches*2？
00:32:53	何川:	嗯
00:32:56	k:	老师，稍微解释下交叉熵吧
00:32:58	孙宝策:	one-hot？
00:33:05	Eric:	vector的长度？
00:33:15	何川:	*2 是不是也可以用softmax？
00:34:38	Eric:	p，q是什么？
00:38:11	Eric:	Softmax是只用在FC网络里面吗？
00:38:26	彭冰:	最后一层的激活函数
00:39:56	Xun Li (Mary):	softmax具体用在哪里啊，pooling之后还是FC之后
00:40:50	Eric:	也就是说只用于最后一层吗？
00:41:00	何川:	torch.nn.logSoftmax+.NLLloss 和 CrosEntropyLoss？这是两种做法是？
00:41:43	Eric:	那FC层用什么激活函数？
00:42:11	邓 奇:	中间层用relu的比较多
00:42:24	Eric:	哦哦，
00:43:01	龙云淋:	超出数值表示范围？
00:43:38	李文艺:	log
00:43:40	龙云淋:	minus max
00:43:41	马玄:	取对数
00:43:47	yangfanqihang:	为啥底数用e
00:43:53	何川:	好像有个做法是都减去最大值
00:45:16	Eric:	“数值不稳定”具体指什么意思？
00:51:26	Eric:	“数值不稳定”是不是因为e是一个大于1的数？
00:52:12	Lenovo:	就是防止那个指数项太大了？
00:52:33	Eric:	感觉是的，
00:54:29	k:	乘以C就不会指数项太大了？C小于1，很小？
00:54:57	k:	哦
00:55:09	邓 奇:	pi*(1-pi)  或者-pi^2
00:55:10	马玄:	pi(1-pi)
00:56:38	k:	j在哪里
01:01:12	龙云淋:	1
01:01:13	何川:	1
01:04:23	k:	怎么会有i不等于j这种情况呢？
01:05:22	张培栋:	分这俩种情况是什么意思
01:06:16	彭冰:	最后一层FC输出两个值记为1，2 对应的softmax输出两个值 比如0.9和0.1 其中 1和0.9 就是i=j的情况 2和0.9就是i不等于j的情况
01:12:54	张培栋:	稀疏向量吗？
01:14:01	k:	为什么会有i不等于j这种情况呢？
01:14:13	马玄:	结果是pi-yi?还是pj-yi?
01:14:16	李晶:	-yj?
01:15:36	WW BB:	9:10回来
01:15:46	k:	老师，这个i不等于j能不能讲下，看不懂啊
01:17:05	郑兴纯:	https://www.cnblogs.com/zongfa/p/8971213.html
01:17:25	郑兴纯:	可以参考看下里面的图 就知道i 和j的问题了
01:21:26	严广宇:	一直没声音？
01:21:56	邓 奇:	休息时间
01:22:09	WW BB:	6:10
01:22:12	WW BB:	9:10
01:24:35	严广宇:	1
01:24:37	邓 奇:	1
01:24:38	唐陈:	1
01:24:39	高格格:	1
01:24:42	韦俊:	11
01:24:46	杨俊桔:	1
01:28:10	侠航 陈:	？
01:28:12	蒋开文:	听不见
01:28:12	yangfanqihang:	？
01:28:15	王睿:	1
01:28:16	高格格:	好了
01:28:16	严广宇:	1
01:28:17	张培栋:	1
01:28:17	包颖:	1
01:28:17	韦俊:	好了
01:28:18	杨俊桔:	1
01:28:19	龙云淋:	1
01:28:22	chenjingwei:	刚刚没有
01:30:35	何川:	0
01:30:36	严广宇:	1
01:30:37	Lenovo:	1
01:30:42	周宏雷:	0
01:30:45	王睿:	PyTorch
01:30:46	张磊:	pytorch
01:30:47	Eric:	我，
01:30:53	刘权:	用pytorch
01:31:11	王睿:	Caffe环境要求英伟达显卡…
01:31:39	马玄:	多讲一些Pytorch
01:32:15	侠航 陈:	1
01:34:47	张磊:	1
01:34:49	马玄:	1
01:34:51	Lenovo:	1
01:36:13	张磊:	这是哪个版本呀？
01:38:28	林汉斌:	后面赵老师还会讲吗
01:38:35	WW BB:	hui
01:39:20	林汉斌:	好
01:40:07	Eric:	您是说“one-hot”吗？
01:43:23	Eric:	TA正在讲的代码是项目里面提供的吗？
01:43:29	Tianhu Zhang:	1
01:43:30	Lenovo:	对
01:43:33	王睿:	是
01:43:40	Eric:	哦哦，谢谢，
01:43:49	Lenovo:	1
01:43:50	Eric:	不太懂，
01:43:50	孙宝策:	1
01:44:06	Eric:	听懂了20%，
01:44:08	李晶:	getitem能再讲讲吗？
01:44:19	张磊:	这是哪个pytorch版本？
01:44:31	Eric:	主要是没有预习，
01:44:38	张磊:	奥，好的
01:45:51	李晶:	明白了
01:46:06	何川:	getitem是返回的是一个样本吗？
01:46:17	王睿:	项目的代码可以有注释吗
01:46:51	Eric:	课下怎么联系您呢？
01:47:15	Eric:	您是哪个班的？
01:47:19	Eric:	哦哦，
01:48:28	Eric:	resize是采样操作吗？
01:48:58	Eric:	哦哦，谢谢，
01:49:42	Eric:	为啥要翻转呢？
01:49:50	何川:	采用torchvision。transform，输入sample是个字典为不是image的数据，不需要转换吗？
01:49:59	邓 奇:	水平翻转是直接替换掉原来图片吗，还是生成新的图片？
01:50:18	k:	翻转是咋翻转的？
01:50:38	张培栋:	镜像吗？
01:50:42	Eric:	哦哦，是不是每张图片都翻转呀？
01:50:57	Eric:	哦哦，
01:51:02	张磊:	翻转了，替代原图吧？
01:51:14	张培栋:	一般是0.5吗？还是根据实际情况？
01:51:17	郑兴纯:	这里的resize插值方式适合mutils里的imutils.resize一样吗
01:51:30	郑兴纯:	imutils
01:51:36	Eric:	既然样本数据少，为什么不干脆每个都翻转呢？
01:52:16	王睿:	PIL不是不支持python3了吗
01:52:32	Eric:	PIL是什么？
01:52:35	王睿:	ok
01:52:36	何川:	也是pillow, 可以用的
01:52:46	张磊:	老师是Linux系统么？
01:53:38	Eric:	PIL这个网页能粘贴在聊天系统里面吗？
01:54:28	Tianhu Zhang:	https://pillow.readthedocs.io/en/stable/
01:55:30	严广宇:	shuffle是啥
01:55:42	龙云淋:	batch_size的设置有什么讲究么？
01:57:50	Xun Li (Mary):	数据增强以后生成的新图对应annotation会自动copy原图annotation吗
01:58:44	Eric:	它是会随机传入翻转数据，
02:00:50	李文艺:	哪个快捷键看的源码呢？
02:00:54	龙云淋:	这个网络结构是基于什么原则设计的呢？或者说，针对一个实际问题，怎么设计一个合适的网络结构呢？
02:00:56	张磊:	ctrl
02:01:21	张磊:	这是自己设计的网络？
02:01:27	张磊:	奥~
02:01:34	何川:	inplace是怎样的操作？
02:01:35	Eric:	inplace设置具体什么意思？
02:02:02	张磊:	relu4是啥意思？
02:02:22	张磊:	好的，知道了
02:02:55	何川:	nn.PReLU()与nn.ReLU()的区别？示例中定义了很多nn.PReLU()，能否只定义一个PReLU？
02:03:51	何川:	这个是人脸检测的一个小问题
02:04:12	龙云淋:	不同的prelu参数不同，不能用同一个
02:04:17	Eric:	这个是动物分类的代码吧，
02:08:16	小天-king-北京-1年:	可以同时写两张显卡吗？
02:09:51	彭冰:	老师 多任务分类loss定义为两个分类器的交叉熵的和 不用去训练他们各自的权重吗 这样的话会不会对loss有偏向性啊 我的训练结果发生明显的偏向
02:11:41	张磊:	代码很简洁~
02:11:44	张磊:	赞
02:13:02	侠航 陈:	这里有涉及到数据不平衡问题吗
02:14:42	蒋开文:	怎么固定feature net
02:15:19	Tang JM:	迁移学习
02:15:46	Eric:	再说一遍最后那个固定什么的…
02:15:49	张磊:	代码会给么
02:15:55	蒋开文:	怎么固定feature net
02:16:36	Tianhu Zhang:	请问这里的分类准确率大概多少合理呢，我用这个结构训练得准确率有点低= =。
02:17:26	Eric:	epo是什么意思？
02:17:56	何川:	怎么earlystop
02:18:06	Eric:	一般轮多少圈？
02:18:13	何川:	好像找不到
02:18:35	何川:	跑多少圈看earlystop
02:18:39	Eric:	哦哦，
02:19:44	龙云淋:	谢谢
02:19:52	郑兴纯:	1
02:19:53	陈悦:	1
02:19:53	邓 奇:	1
02:19:53	徐楠:	1
02:19:53	高格格:	1
02:19:53	laichengxin:	1
02:19:54	张培栋:	epoch1
02:19:55	张磊:	1
02:21:07	Eric:	pytorch修改网络结构是不是比TF简单？
02:21:37	WW BB:	10:15
02:21:44	彭冰:	赵老师 像多任务分类问题 目前比较成熟的做法 是如何去计算loss的呢 是固定部分网络参数吗 还是有其他更好的方法
02:22:15	WW BB:	这个后面咱们就讲哈
02:22:21	彭冰:	哦哦 好
02:22:39	Eric:	pytorch修改网络结构是不是比TF简单一些？
02:23:39	彭冰:	这个好像跟静态图动态图有关系 TF2好像全都支持了 TF1好像只支持静态图 我记不太清了
02:26:50	Jia Endong:	from pytorchtools import EarlyStopping, pytorch里要自己写
02:29:32	何川:	哦哦，谢谢
02:29:32	徐楠:	1
02:29:33	邓 奇:	1
02:36:08	Song:	获取数据吗?
02:38:33	李文艺:	weighted cross entropy loss 在pytorch有api实现吗？
02:38:54	李文艺:	还是需要自己写呢
02:39:23	Xun Li (Mary):	少的数据赋更大的weight吗
02:40:02	李文艺:	那参数怎么对应类别呢
02:46:49	Xun Li (Mary):	刚才贾老师说的固定feature net就是这种吗
02:46:49	严广宇:	 每个小网络相当于二分类？
02:48:38	Eric:	九头鸟。
02:49:42	Lenovo:	妙
02:49:57	罗家伟:	数据增广不是一开始就进行的嘛
02:49:58	邓 奇:	先拿第一个任务训练好backbone中参数，然后固定好之后再分别训练每一个heads中参数？
02:50:29	何川:	一般backbone和head的层数怎么比例？
02:50:55	Eric:	听说过ResNet，
02:51:38	李文艺:	有没有这块的代码的例子呢？
02:52:12	何川:	github上找googglenet的网络
02:52:15	张培栋:	baseline是啥意思
02:52:29	Eric:	Pretrained是不是肯定没有自己训练的效果好？
02:52:35	何川:	一般backbone和head的层数怎么比例呢？
03:01:34	张培栋:	1
03:01:37	Lenovo:	能
03:01:55	Xun Li (Mary):	怎么感觉像k-means?
03:02:12	李晶:	Cyi又是怎么算呢？
03:02:14	Tianhu Zhang:	中心怎么来的呢
03:04:39	龙云淋:	哪一层的特征？
03:06:44	李文艺:	lamada的值是多少呢？
03:07:00	邓 奇:	刚才的下标2是啥意思？
03:07:32	郑兴纯:	2范数
03:07:42	何川:	有点类似正则化
03:11:13	张培栋:	行为检测在用
03:15:07	马玄:	感觉重在实战
03:15:09	徐楠:	1
03:15:20	Eric:	Attention在目标检测里面用的多吗？
03:15:36	王睿:	最后这三个项目是？会给到吗？
03:16:47	高格格:	项目一是不是比项目二简单呀？
03:16:56	林汉斌:	attention是如何关注到眼镜的，而不关注的其他的，是跟loss有关吗
03:17:39	Eric:	pytorch修改网络结构是不是比TF简单？
03:17:51	孟明:	branch那个网上资料搜不到，要搜什么关键字？
03:17:54	彭冰:	老师能不能再讲解一下 之前backbone + branch那里 怎么保证训练其余的branch时 不会破坏之前brach的训练参数的
03:17:57	Eric:	哦哦，
03:18:39	彭冰:	好
03:19:10	孟明:	使用branch2进行预测的时候，branch1的参数不起作用？
03:19:11	Xun Li (Mary):	之前贾老师的示例提到固定feature net，可以看作backbone+branch吗
03:19:16	Eric:	要不再讲讲Caffe吧，
03:19:40	王睿:	Caffe不是会发链接吗
03:19:45	Eric:	前面那个地方，
03:19:51	Lenovo:	发链接就好
03:20:11	Eric:	哦哦，
03:20:20	李文艺:	weight CE loss 的参数怎么用的呢
03:20:39	李文艺:	参数不知道怎么输入
03:21:01	Eric:	Pytorch可以自定义loss函数吗？
03:21:18	Eric:	哦哦，
03:21:27	高格格:	老师再见
03:21:32	Xun Li (Mary):	谢谢两位老师
03:21:36	邓 奇:	谢谢老师
03:21:42	徐楠:	谢谢两位老师
